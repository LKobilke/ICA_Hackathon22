[["index.html", "ICA Hackathon 2022 General information What can I learn from this tutorial?", " ICA Hackathon 2022 Lara Kobilke, Department of Media and Communication (IfKW), Ludwig-Maximilians University Munich 2022-06-07 General information This online tutorial accompanies the the first ever ICA Hackathon (2022, Paris). It has been designed to guide you through the breakout session An introduction to tidy text analysis with R held by Lara Kobilke. All materials (e.g., data files) will be uploaded to my Github page. What can I learn from this tutorial? After completing the entire tutorial, you will have acquired two important skills: Know how to use the tidyverse to complete your data management. Know how to use the tidy approach for basic text analysis. Each tutorial consists of: an introduction to new R functions and analysis methods important take-aways to remember links to additional sources of information If we have time left, you can practice your new skills on a real data set (Twitter data about the Russian aggression against Ukraine). "],["tutorial-installing-understanding-rr-studio.html", " 1 Tutorial: Installing &amp; Understanding R/R Studio 1.1 Installing R 1.2 Installing R Studio 1.3 Updating R and R Studio 1.4 Packages 1.5 Take-Aways 1.6 Additional tutorials", " 1 Tutorial: Installing &amp; Understanding R/R Studio After working through Tutorial 1, youll have set up / updated R &amp; RStudio know how to install and activate R packages Please make sure you have the most recent versions of R and RStudio installed before we begin. 1.1 Installing R If you have never installed R before, please watch one of these two video instructions: Video Tutorial for Windows Video Tutorial for Mac When you are ready to install R, use Cran to install the newest version of R (4.2.0, Vigorous Calisthenics). Youll have to specify your operation system to download the right version: Installer for Windows Installer for Mac 1.2 Installing R Studio Install R Studio next. R Studio is a graphical interface that makes programming with R much easier. The newest version of R Studio (2022.02.2+485) can be downloaded via this Link. 1.3 Updating R and R Studio If you have already installed R and RStudio, please update your version to the latest version. This way, well all know that our versions are compatible. 1.3.1 On Windows Updating on Windows is tricky. Therefore, you can use a package called installr, which helps you manage your update. First, install the installr package. Use the following code in the RGui console (not RStudio!): # installing/loading the package: if(!require(installr)) { install.packages(&quot;installr&quot;); require(installr) } #load / install+load installr After you have installed or loaded the installr package, lets start the updating process of your R installation by using the updateR() function. It will check for newer versions, and if one is available, will guide you through the decisions youd need to make: # using the package: updateR() Finally, update R Studio. Updating RStudio is easy, just open RStudio and go to Help &gt; Check for Updates to install a newer version. 1.3.2 On MAC Go to CRAN and install the newer package installer. After that update R Studio. Updating RStudio is easy, just open RStudio and go to Help &gt; Check for Updates to install a newer version. 1.4 Packages While Base R, i.e., the standard version of R, already includes many helpful functions, you may at times need other, additional functions. For example, if we want to perform tidy text analysis in R well need to use specific packages including additional functions. Packages are sets of topic-specific functions that build on the Base R functions.. 1.4.1 Installing packages To use a package, you have to install it first. Lets say youre interested in using the data management package dplyr. You may install the package on your machine with the command install.packages(), but you have to provide the name of the package you want to install. install.packages(&quot;dplyr&quot;) The package is now installed and accessible on your computer. We just need to call install.packages() once for any package. After that, all you have to do is open R and activate the package that has already been installed. 1.4.2 Activating packages A package must be activated in each session before it is used in the code. As a result, before running your code, you should activate the packages that you require: Use the library()_ command for this by providing the name of the package that you want to activate: library(dplyr) You can also use the name of the package followed by two colons :: to activate a package directly before calling one of its function. For instance, I do not need use to activate the dplyr package (by using the library() function) to use the function summarize() if I use the following code: dplyr::summarize() 1.4.3 Getting information about packages How do we utilize the package now that its installed and activated? You can get an overview of a packages functions by consulting its corresponding reference manual or, if available, its vignette (tutorials on how to use selected functions for the corresponding package) provided by the packages author on a website called CRAN. Google is the best place to look for these manuals/vignettes: For example, if you google CRAN ProcessR, youll find the following website: Image: Cran Overview dplyr package The first paragraph (red-circled region) provides an overview of potential uses for this package. The reference manual and the vignette are linked in the second red-circled region. You can go through the reference documentation to get a sense of the different functions available in the dplyr package. 1.5 Take-Aways Installing: install R &amp; RStudio via CRAN Packages: expand Base R via install.packages() and activate the packages via library() (each session!) 1.6 Additional tutorials You still have questions? The following tutorials &amp; papers can help you with that: YaRrr! The Pirates Guide to R by N.D. Phillips, Tutorial 2 R Cookbook by Long et al., Tutorial 1 Now that you know the layout of R, we can get started with some real action: [Tutorial: Using R as a calculator] "],["tutorial-first-steps-in-r.html", " 2 Tutorial: First steps in R 2.1 Using R as a calculator 2.2 Using variables for calculation 2.3 Tutorial: Working with data (files) 2.4 Take-Aways 2.5 Additional tutorials", " 2 Tutorial: First steps in R After working through Tutorial 2, youll be able to work with mathematical operators in R understand how to import data know how to select variables and values in data frames 2.1 Using R as a calculator One of the first things everyone learns in R is to use R as a calculator. You have access to many mathematical operators in R (e.g. +, -, *, /, ^). Lets try some of them. Addition: 5+7 ## [1] 12 Subtraction: 12-7 ## [1] 5 Exponentiation: 3^3 ## [1] 27 2.2 Using variables for calculation You can also assign numbers to variables with the assign operator &lt;-. Please remember that a variable name in R can include numeric and alphabets along with special characters like dot (.) and underline (_). Therefore, these are good options to name your variables: my_1st_number &lt;- 3 my.1st.numer &lt;- 3 You can use variables in your calculations by assigning the numbers to variables (i.e. store the numerical value in the variable). five &lt;- 5 seven &lt;- 7 twelve &lt;- five + seven # here you add the two variables in which the numbers are stored. The result of the addition is stored in the variable &quot;twelve&quot; twelve # now you have to retrieve the content of the variable, so that the result is printed to the console ## [1] 12 2.2.1 Using vectors for calculation You can also store more than one number in a variable. We call this process creating vectors because variables that contain more than one number are called vectors in R. Vectors are created using the combine function c() in R. twelve &lt;- c(1,2,3,4,5,6,7,8,9,10,11,12) twelve # print the content of the variable to the console ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 Again, the variable name is chosen arbitrarily. You can also do this: twelve &lt;- c(4,10,15,21,33) twelve # print the content of the variable to the console ## [1] 4 10 15 21 33 You can use mathematical operations on vectors (e.g., +, -, * and /). Lets create two vectors weight and height that contain the weight and height measures of 6 individuals. For example, the first individual weighs 60 kg and is 1.75 m tall: weight &lt;- c(60, 72, 57, 90, 95, 72) height &lt;- c(1.75, 1.80, 1.65, 1.90, 1.74, 1.91) Now we can calculate the Body Mass Index (BMI) using the BMI formula: BMI &lt;- weight/height^2 BMI # print the content of the BMI variable to the console ## [1] 19.59184 22.22222 20.93664 24.93075 31.37799 19.73630 Now we know that the first person has a BMI of 19.59, which is within the range of normality (18.5 and 24.9). 2.2.2 Selecting values from a vector We still see the BMI of all the other five people, i.e. the entire vector. How can we select only the first person? You can select values from a vector by using square brackets [ ] and enter the number of the entry that you want to print to your console. BMI[1] ## [1] 19.59184 Again, you can see that the first person has a BMI of 19.59. You could also decide to look at all values except the first one: BMI[-1] ## [1] 22.22222 20.93664 24.93075 31.37799 19.73630 2.3 Tutorial: Working with data (files) 2.3.1 Defining your working directory In most cases, you do not want to manually enter all your values into R and combine them with the c() function. Instead, you want to import data files that you already have on your drive/personal computer. The first step to importing your data into R is to define your working directory. Your working directory is the folder from which data can be imported into R or to which you can export and save data created with R. Create a folder that you want to use as your working directory for this tutorial (or use an existing one, that also works). For example, Ive created a folder called Hackathon22. Go to that folder and copy the path to it: Image: Working Directory on Windows Image: Copy Working Directory on Windows On Mac, you go to your folder and right click on it. An options menu opens and you can copy the folder path: Image: Copy Working Directory on MAC Now you know where this working directory is located - but R should know, too! Telling R from which folder to import data or where to export data to is also called setting your working directory. We call a function called setwd() (you guessed right: short for setting you working directory) which allows us to do exactly that. Important: The way this working directory is set differs between Windows- and Mac-Operating Systems. Windows: The dashes need to be pointing towards the right direction (if you simply copy the path to the folder, you may need to replace these signs \\ with /) setwd(&quot;C:/Users/LaraK/Documents/Hackathon22&quot;) Mac: You may need to add a / at the beginning like so: setwd(&quot;/Users/LaraK/Documents/Hackathon22&quot;) If you have forgotten where you set your working directory, you can also ask R about the path of your current working directory with getwd(): getwd() ## [1] &quot;C:/Users/LaraK/Documents/Hackathon22&quot; 2.3.2 Import data from your working directory After setting the working directory, you need to transfer the data file that you want to work with to that folder (here: the IPR folder). Download the data_tutorial3.csv from Moodle, i.e., the 4. Mai material folder. The data set consists of data that is completely made up - a survey with 20 fictional students in a fictional seminar. We only use this data here so you can understand differences between types of data and types of objects. The data file data_tutorial3.csv is structured as follows: Each row contains the answer for a single student. Each column contains all values given by students for a single variable. The variables included here are: name: the name of each student age: the age of each student (in years) date: the date on which each student was surveyed (in YYYY-MM-DD) outlet: the type of media outlet each students mainly uses to get information outlet_use: the time each student uses this media outlet daily (in hours) outlet_trust: how much each student trusts this media outlet (from 1 = not at all to 5 = very much) Well read in the file with read.csv(). Here, we specify where to find the data file with the file path in quotation marks, but you dont need to specify that path if you have already set it as your working directory. In addition, we provide an argument to let R know that the first row contains variable names with the argument header = TRUE. In the end, we assign our data file to a source object that we call survey. The data is now stored in this object. While read.csv() reads in comma-separated values, read.csv2() reads in values that are separated by semicolons. survey &lt;- read.csv2(&quot;data_tutorial3.csv&quot;, header = TRUE) survey ## X.2 X.1 X name age date outlet outlet_use outlet_trust ## 1 1 1 1 Alexandra 20 2021-09-09 TV 2 5 ## 2 2 2 2 Alex 25 2021-09-08 Online 3 5 ## 3 3 3 3 Maximilian 29 2021-09-09 Zeitung 4 1 ## 4 4 4 4 Moritz 22 2021-09-06 TV 2 2 ## 5 5 5 5 Vanessa 25 2021-09-07 Online 1 3 ## 6 6 6 6 Andrea 26 2021-09-09 Online 3 4 ## 7 7 7 7 Fabienne 26 2021-09-09 TV 3 2 ## 8 8 8 8 Fabio 27 2021-09-09 Online 0 1 ## 9 9 9 9 Magdalena 8 2021-09-08 Online 1 4 ## 10 10 10 10 Tim 26 2021-09-07 TV NA 2 ## 11 11 11 11 Alex 27 2021-09-09 Online NA 2 ## 12 12 12 12 Tobias 26 2021-09-07 Online 2 2 ## 13 13 13 13 Michael 25 2021-09-09 Online 3 2 ## 14 14 14 14 Sabrina 27 2021-09-08 Online 1 2 ## 15 15 15 15 Valentin 29 2021-09-09 TV 1 5 ## 16 16 16 16 Tristan 26 2021-09-09 TV 2 5 ## 17 17 17 17 Martin 21 2021-09-09 Online 1 2 ## 18 18 18 18 Anna 23 2021-09-08 TV 3 3 ## 19 19 19 19 Andreas 24 2021-09-09 TV 2 5 ## 20 20 20 20 Florian 26 2021-09-09 Online 1 5 2.4 Take-Aways Mathematical operators: use +, -, *, /, ^ on numbers, variables, and vectors Create vectors: use the combine function c() Select values from vectors: use square brackets [ ] Setting the working directory: tells R where your folder with the data is located on your drive, setwd(your_filepath) Import data: after setting the working directory, with read.csv() (comma-separated) or read.csv2() (semicolon-separated) 2.5 Additional tutorials You still have questions? The following online guides can help you with that: Using R as a Calculator R Vector Import CSV Files into R Step-by-Step Guide Subsetting data Indexing into a data structure "],["tutorial-data-management-with-tidyverse.html", " 3 Tutorial: Data management with tidyverse 3.1 Why not stick with Base R? 3.2 Tidyverse packages 3.3 Tidy data 3.4 The pipe operator 3.5 Data transformation with dplyr 3.6 Take-Aways 3.7 Additional tutorials", " 3 Tutorial: Data management with tidyverse After working through Tutorial 3, youll know the advantages of the tidyverse vs. Base R know about different formats of tabular data understand what packages are included in the tidyverse meta-package know how to do data modifications and transformations with dplyr 3.1 Why not stick with Base R? You might wonder why weve spent so much time exploring functions in Base R to now learn data management with tidyverse. After all, data management can also be done in Base R, cant it? I personally recommend that all R beginners should work with the tidyverse as early as possible. There are three reasons supporting my argument: Ease of use: The tidyverse is very accessible for R beginners, i.e. its syntax is very easy to understand. It allows you to set goals (i.e. what you want to do with your data) and get you working on these goals very quickly. Definitely more quickly than in Base R! Standard for data management: A few years ago, the tidyverse has become the de facto standard for data management in R. It is a meta-package, which means that it is a collection of distinct packages that all follow the same design principles to make code reading and writing as simple as possible. For example, all functions are named after verbs that indicate exactly what they perform (e.g. filter or summarize). Beautiful graphs: With the tidyverse, all data management steps can be swiftly transferred into beautiful graphs. This is because the most popular graph package in R, ggplot2, is part of the tidyverse. Are you excited now? Then lets get started! 3.2 Tidyverse packages The tidyverse comes with a great arsenal of topic-specific packages and their respective functions. It includes packages for: tibble: creating data structures like tibbles, which is an enhanced type of data frame readr, haven, readxl: reading data (e.g. readr for CSV, haven for SPSS, Stata and SAS, readxl for Excel) tidyr, dplyr: data transformation, modification, and summary statistics stringr, forcats, lubridate: create special, powerful object types (e.g. stringr for working with text objects, forcats for factors, lubridate for time data) purrr: programming with R ggplot2: graphing/charting The most frequently used packages of the tidyverse can be installed and activated in one go (less frequently used packages like haven still need to be installed and activated separately): install.packages(&quot;tidyverse&quot;) # install the package (only on the first time) library(tidyverse) # active the package 3.3 Tidy data Dataframes are tabular data. However, data can also have other formats, for example as nested, i.e. hierarchical, lists. In communication research, these other data formats are mainly used by social media and their respective APIs (perhaps you have heard of the JSON format before). In our course, however, well focus on tabular data.The same data can be represented differently in tables. We perceive some of these representations as tidy, others as messy. While tidy data principles establish a standard for organizing data values inside a data frame and thus all tidy data look the same, every messy dataset is messy in its own way. Take a look at the table below. It shows a Starwars data set that comes pre-installed with the dplyr package. Do you feel the tabled data is messy? Why (not)? ## # A tibble: 10 x 3 ## name body_feature value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Anakin Skywalker height 188 ## 2 Anakin Skywalker mass 84 ## 3 Chewbacca height 228 ## 4 Chewbacca mass 112 ## 5 Darth Vader height 202 ## 6 Darth Vader mass 136 ## 7 Jabba Desilijic Tiure height 175 ## 8 Jabba Desilijic Tiure mass 1358 ## 9 Leia Organa height 150 ## 10 Leia Organa mass 49 Overall, this data is messy. It comes with three messy problems: This body_feature column comprises information relating to both height and weight, i.e. both variables are stored in a single column. As a result, the value column is reliant on the body_feature column; we cant tell the stored values apart by merely looking at the value column. We always need to check the body_feature column. Consequently, we have issues with vectorized functions (remember, in R, columns in data sets are vectors): We cant, for example, use the mean() function on the value column to determine the average weight of the Star Wars characters since the height values are also stored there. What do you think of this table? Is it messy? ## # A tibble: 5 x 3 ## name height mass ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Anakin Skywalker 188 84 ## 2 Chewbacca 228 112 ## 3 Darth Vader 202 136 ## 4 Jabba Desilijic Tiure 175 1358 ## 5 Leia Organa 150 49 This table looks tidy! Tidy data is a standard way of mapping the meaning of a dataset to its structure. We determine whether a dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. We consider a table tidy when it follows the following golden rules: Columns: Every column is one variable. Rows: Every row is one observation. Cells: Every cell contains one single value. Image: The tidy data principle (Source: R for Data Science) In messy data sets, on the other hand Column headers are values, not variable names. Multiple variables are stored in one column. Variables are stored in both rows and columns. Multiple types of observational units are stored in the same table. A single observational unit is stored in multiple tables. Why should you be concerned about tidy data organization? There are two major advantages: When you have a consistent data structure, it is easier to learn the respective tools that work well with this data structure. dplyr, ggplot2, and all the other tidyverse packages are designed for working with tidy data. Putting variables in columns makes Rs vectorized nature shine. The majority of built-in R-functions (like the mean() function) works with vectors of values. As a result, the tidy reorganization of data seems only natural for a good work flow in R. If you have been working mainly with survey data, then you will already be familiar with these basic rules, as data export from survey software usually follows these principles. However, real-world data from databases or social media often does not follow these principles. Thats why its sometimes true to say that 80% of data analysis is spent on cleaning and transforming data. 3.4 The pipe operator Truly, dplyr is my favorite tidyverse package (even more so than ggplot2, which well cover later!). It allows you to perform powerful data transformations in just a few simple steps. To this end, dplyr relies on the pipe operator (%&gt;%).1 The %&gt;% operator allows functions to be applied sequentially to the same source object in a concise manner, so that step-by-step transformations can be applied to the data. Therefore, we always call the source object first and then add each transformation step separated by the %&gt;% operator. Lets illustrate this concept with an example. Well use the Starwars data set that you are already familiar with. starwars_data %&gt;% # First, we define the source object, i.e. the data frame that we want to transform, followed by the pipe operator plot() # Second, we specify which function should be performed on the source object, here: scatterplot Now, thats not very impressive. We could do the same in Base R like this: plot(starwars_data) However, dplyr gets really impressive when you chain functions sequentially. You can apply certain selection criteria to your data and plot it in one go. For example, we might exclude the variable name from our scatter plot, since its not a metric variable anyway. Also, we might want to look only at those Star Wars characters taller than 170 cm. Lets try it in a single run! starwars_data %&gt;% # Define the source object select(height, mass) %&gt;% # Keep only the height and mass column filter(height &gt; 170) %&gt;% # Filter all observations that are taller than 170cm plot() # Plot! Now try to do the same in Base R: plot(starwars_data[starwars_data$height&gt;170,]$mass~starwars_data[starwars_data$height&gt;170,]$height, xlab=&quot;height&quot;, ylab=&quot;mass&quot;) The Base R code is longer, more nested, and not as readable as the code written in dplyr. And the more selection criteria and functions you need to implement, the worse it gets. For example, imagine you would also want to exclude Star Wars characters with a mass bigger than 1200kg. Peace of cake with dplyr: starwars_data %&gt;% select(height, mass) %&gt;% filter(height &gt; 170) %&gt;% filter(mass &lt; 1200) %&gt;% plot() 3.5 Data transformation with dplyr dplyrcomes with five main functions: select(): select variables column by column, i.e. pick columns / variables by their names filter(): filter observations row by row, i.e. pick observations by their values arrange(): sort / reorder data in ascending or descending order mutate(): calculate new variables or transform existing ones summarize(): summarize variables (e.g. mean, standard deviation, etc.), best combined with group_by() 3.5.1 select() Scientists will frequently provide you with large data sets including hundreds of variables (often even more!). The first problem in this scenario is narrowing down the variables you are truly interested in. select() helps you to easily choose a suitable subset of variables. In this selection process, the name of the data frame is the source object, followed by the pipe %&gt;% operator. The expression that selects the columns that you are interested in comes after that. Take the Star Wars data, for example. The original data set has 87 observations (Star Wars characters) and 14 columns / variables (traits of these characters, e.g., birth_year, gender, and species). Yes, 14 columns is not a lot and you could get an overview of this data without subsetting columns. Lets take a look at the original data frame: library(dplyr) # load dplyr starwars_data &lt;- starwars # assign the pre-installed starwars data from dplyr to a source object / variable starwars_data # print the content of the data frame to the console ## # A tibble: 87 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender homeworld species films vehicles ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lis&gt; &lt;list&gt; ## 1 Luke Sky~ 172 77 blond fair blue 19 male mascu~ Tatooine Human &lt;chr&gt; &lt;chr&gt; ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu~ Tatooine Droid &lt;chr&gt; &lt;chr&gt; ## 3 R2-D2 96 32 &lt;NA&gt; white, bl~ red 33 none mascu~ Naboo Droid &lt;chr&gt; &lt;chr&gt; ## 4 Darth Va~ 202 136 none white yellow 41.9 male mascu~ Tatooine Human &lt;chr&gt; &lt;chr&gt; ## 5 Leia Org~ 150 49 brown light brown 19 fema~ femin~ Alderaan Human &lt;chr&gt; &lt;chr&gt; ## 6 Owen Lars 178 120 brown, gr~ light blue 52 male mascu~ Tatooine Human &lt;chr&gt; &lt;chr&gt; ## 7 Beru Whi~ 165 75 brown light blue 47 fema~ femin~ Tatooine Human &lt;chr&gt; &lt;chr&gt; ## 8 R5-D4 97 32 &lt;NA&gt; white, red red NA none mascu~ Tatooine Droid &lt;chr&gt; &lt;chr&gt; ## 9 Biggs Da~ 183 84 black light brown 24 male mascu~ Tatooine Human &lt;chr&gt; &lt;chr&gt; ## 10 Obi-Wan ~ 182 77 auburn, w~ fair blue-gray 57 male mascu~ Stewjon Human &lt;chr&gt; &lt;chr&gt; ## # ... with 77 more rows, and 1 more variable: starships &lt;list&gt; For the sake of practice, lets say we only want to analyze the species, birth_year, mass, and height of these characters. To simplify data handling, we want to keep only the respective columns. starwars_data %&gt;% # define the source object select(name, species, birth_year, mass, height) # keep only the name, species, birth_year, mass and height column ## # A tibble: 87 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 C-3PO Droid 112 75 167 ## 3 R2-D2 Droid 33 32 96 ## 4 Darth Vader Human 41.9 136 202 ## 5 Leia Organa Human 19 49 150 ## 6 Owen Lars Human 52 120 178 ## 7 Beru Whitesun lars Human 47 75 165 ## 8 R5-D4 Droid NA 32 97 ## 9 Biggs Darklighter Human 24 84 183 ## 10 Obi-Wan Kenobi Human 57 77 182 ## # ... with 77 more rows At the moment you have only printed the transformed data to the console. However, most of the time we want to keep the transformed data ready for further calculations. In this case we should assign the transformed data into a new source object, which we can access later. starwars_short &lt;- starwars_data %&gt;% # assign a new source object and define the old source object select(name, species, birth_year, mass, height) # keep only the name, species, birth_year, mass and height column Lets print the new source object, starwars_short, to the console. starwars_short ## # A tibble: 87 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 C-3PO Droid 112 75 167 ## 3 R2-D2 Droid 33 32 96 ## 4 Darth Vader Human 41.9 136 202 ## 5 Leia Organa Human 19 49 150 ## 6 Owen Lars Human 52 120 178 ## 7 Beru Whitesun lars Human 47 75 165 ## 8 R5-D4 Droid NA 32 97 ## 9 Biggs Darklighter Human 24 84 183 ## 10 Obi-Wan Kenobi Human 57 77 182 ## # ... with 77 more rows You can also delete columns by making a reverse selection with the - symbol. This means that you select all columns except the one whose name you specify. starwars_short %&gt;% select(-name) # keep all columns except the name column (i.e. delete name column) ## # A tibble: 87 x 4 ## species birth_year mass height ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Human 19 77 172 ## 2 Droid 112 75 167 ## 3 Droid 33 32 96 ## 4 Human 41.9 136 202 ## 5 Human 19 49 150 ## 6 Human 52 120 178 ## 7 Human 47 75 165 ## 8 Droid NA 32 97 ## 9 Human 24 84 183 ## 10 Human 57 77 182 ## # ... with 77 more rows You can delete more than one column in one go: starwars_short %&gt;% select(-c(name,species)) # keep all columns except the name &amp; species column (i.e. delete these columns) ## # A tibble: 87 x 3 ## birth_year mass height ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 19 77 172 ## 2 112 75 167 ## 3 33 32 96 ## 4 41.9 136 202 ## 5 19 49 150 ## 6 52 120 178 ## 7 47 75 165 ## 8 NA 32 97 ## 9 24 84 183 ## 10 57 77 182 ## # ... with 77 more rows Tip for advanced users: You can select columns and rename them at the same time. starwars_short %&gt;% select(&quot;character&quot;=name, &quot;age&quot;=birth_year) # select columns that you want to keep &amp; rename them ## # A tibble: 87 x 2 ## character age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Luke Skywalker 19 ## 2 C-3PO 112 ## 3 R2-D2 33 ## 4 Darth Vader 41.9 ## 5 Leia Organa 19 ## 6 Owen Lars 52 ## 7 Beru Whitesun lars 47 ## 8 R5-D4 NA ## 9 Biggs Darklighter 24 ## 10 Obi-Wan Kenobi 57 ## # ... with 77 more rows 3.5.2 filter() filter() divides observations into groups depending on their values. The name of the data frame is the source object, followed by the pipe %&gt;% operator. Then follow the expressions that filter the data. Lets only select human Star Wars characters in our transformed data set starwars_short: starwars_short %&gt;% filter(species==&quot;Human&quot;) ## # A tibble: 35 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 Darth Vader Human 41.9 136 202 ## 3 Leia Organa Human 19 49 150 ## 4 Owen Lars Human 52 120 178 ## 5 Beru Whitesun lars Human 47 75 165 ## 6 Biggs Darklighter Human 24 84 183 ## 7 Obi-Wan Kenobi Human 57 77 182 ## 8 Anakin Skywalker Human 41.9 84 188 ## 9 Wilhuff Tarkin Human 64 NA 180 ## 10 Han Solo Human 29 80 180 ## # ... with 25 more rows And now lets only select Star Wars character who are younger than 24 or exactly 24 years old. starwars_short %&gt;% filter(birth_year&lt;=24) ## # A tibble: 7 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 Leia Organa Human 19 49 150 ## 3 Biggs Darklighter Human 24 84 183 ## 4 Wedge Antilles Human 21 77 170 ## 5 IG-88 Droid 15 140 200 ## 6 Wicket Systri Warrick Ewok 8 20 88 ## 7 Plo Koon Kel Dor 22 80 188 Chaining some functions, lets look at Star Wars character who are a Droid and older than 24. starwars_short %&gt;% filter(species==&quot;Droid&quot; &amp; birth_year &gt; 24) # &amp; --&gt; filter all observations to which both logical statements apply ## # A tibble: 2 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 Alternatively, you can also write these filters like this: starwars_short %&gt;% filter(species==&quot;Droid&quot;) %&gt;% filter(birth_year &gt; 24) ## # A tibble: 2 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 Besides the &amp; operator, there are many more logical operators that you can choose from to optimize your filter choices. Here is an overview: Image: Logical, i.e. boolean, operators (Source: R for Data Science) []!(C:/Users/LaraK/Desktop/To-Do/ICA Hackathon 2022/LaraKobilke/images/Tut7_logical_operators.PNG) Tip for advanced users 1: You can negate filters. This means that you keep all observations except the one that you have specified with the != operator (read != as: is not or is unequal to). For example, you can choose to include only non-human Star Wars characters. starwars_short %&gt;% filter(species!=&quot;Human&quot;) ## # A tibble: 48 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 ## 3 R5-D4 Droid NA 32 97 ## 4 Chewbacca Wookiee 200 112 228 ## 5 Greedo Rodian 44 74 173 ## 6 Jabba Desilijic Tiure Hutt 600 1358 175 ## 7 Yoda Yoda&#39;s species 896 17 66 ## 8 IG-88 Droid 15 140 200 ## 9 Bossk Trandoshan 53 113 190 ## 10 Ackbar Mon Calamari 41 83 180 ## # ... with 38 more rows Alternatively, you achieve the same goal by negating the entire function call. Negating the entire function call can be handy at times. starwars_short %&gt;% filter(!(species==&quot;Human&quot;)) ## # A tibble: 48 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 ## 3 R5-D4 Droid NA 32 97 ## 4 Chewbacca Wookiee 200 112 228 ## 5 Greedo Rodian 44 74 173 ## 6 Jabba Desilijic Tiure Hutt 600 1358 175 ## 7 Yoda Yoda&#39;s species 896 17 66 ## 8 IG-88 Droid 15 140 200 ## 9 Bossk Trandoshan 53 113 190 ## 10 Ackbar Mon Calamari 41 83 180 ## # ... with 38 more rows Tip for advanced users 2: You can filter for missing values (NAs) with the is.na() function. starwars_short %&gt;% filter(is.na(birth_year)) ## # A tibble: 44 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 R5-D4 Droid NA 32 97 ## 2 Jek Tono Porkins Human NA 110 180 ## 3 Arvel Crynyd Human NA NA NA ## 4 Nien Nunb Sullustan NA 68 160 ## 5 Nute Gunray Neimodian NA 90 191 ## 6 Roos Tarpals Gungan NA 82 224 ## 7 Rugor Nass Gungan NA NA 206 ## 8 Ric Olié &lt;NA&gt; NA NA 183 ## 9 Watto Toydarian NA NA 137 ## 10 Sebulba Dug NA 40 112 ## # ... with 34 more rows And you can negate that filter to get rid of all observation that have missing values (NAs). starwars_short %&gt;% filter(!is.na(birth_year)) ## # A tibble: 43 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Luke Skywalker Human 19 77 172 ## 2 C-3PO Droid 112 75 167 ## 3 R2-D2 Droid 33 32 96 ## 4 Darth Vader Human 41.9 136 202 ## 5 Leia Organa Human 19 49 150 ## 6 Owen Lars Human 52 120 178 ## 7 Beru Whitesun lars Human 47 75 165 ## 8 Biggs Darklighter Human 24 84 183 ## 9 Obi-Wan Kenobi Human 57 77 182 ## 10 Anakin Skywalker Human 41.9 84 188 ## # ... with 33 more rows Tip for advanced users 3: Watch out for the | operator (read: or). This one can be tricky to negate! For example, with this code you get all characters that are NEITHER human NOR older than 33 years. I.e. you get all non-human characters who are younger than 33 or exactly 33 years old. starwars_short %&gt;% filter(!((species == &quot;Human&quot;) | (birth_year &gt; 33))) ## # A tibble: 4 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 R2-D2 Droid 33 32 96 ## 2 IG-88 Droid 15 140 200 ## 3 Wicket Systri Warrick Ewok 8 20 88 ## 4 Plo Koon Kel Dor 22 80 188 But with this code, youll get all observations that are either non-human (regardless of their age) OR humans who are older than 33 years old. starwars_short %&gt;% filter((species != &quot;Human&quot;) | (birth_year &gt; 33)) ## # A tibble: 67 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 C-3PO Droid 112 75 167 ## 2 R2-D2 Droid 33 32 96 ## 3 Darth Vader Human 41.9 136 202 ## 4 Owen Lars Human 52 120 178 ## 5 Beru Whitesun lars Human 47 75 165 ## 6 R5-D4 Droid NA 32 97 ## 7 Obi-Wan Kenobi Human 57 77 182 ## 8 Anakin Skywalker Human 41.9 84 188 ## 9 Wilhuff Tarkin Human 64 NA 180 ## 10 Chewbacca Wookiee 200 112 228 ## # ... with 57 more rows 3.5.3 arrange() arrange() and filter() are like two brothers: both look similar, but they also differ in at least one essential aspect. Both functions change the rows of the data frame, but unlike filter(), arrange() does not select or delete rows, it only changes their order (either ascending or descending). By default, arrange() will sort in ascending order, i.e. from 1:100 (numeric vector) and from A:Z (character vector). arrange() must always be applied to at least one column that is to be sorted. starwars_short %&gt;% arrange(birth_year) ## # A tibble: 87 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Wicket Systri Warrick Ewok 8 20 88 ## 2 IG-88 Droid 15 140 200 ## 3 Luke Skywalker Human 19 77 172 ## 4 Leia Organa Human 19 49 150 ## 5 Wedge Antilles Human 21 77 170 ## 6 Plo Koon Kel Dor 22 80 188 ## 7 Biggs Darklighter Human 24 84 183 ## 8 Han Solo Human 29 80 180 ## 9 Lando Calrissian Human 31 79 177 ## 10 Boba Fett Human 31.5 78.2 183 ## # ... with 77 more rows To get a descending order: starwars_short %&gt;% arrange(desc(birth_year)) ## # A tibble: 87 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Yoda Yoda&#39;s species 896 17 66 ## 2 Jabba Desilijic Tiure Hutt 600 1358 175 ## 3 Chewbacca Wookiee 200 112 228 ## 4 C-3PO Droid 112 75 167 ## 5 Dooku Human 102 80 193 ## 6 Qui-Gon Jinn Human 92 89 193 ## 7 Ki-Adi-Mundi Cerean 92 82 198 ## 8 Finis Valorum Human 91 NA 170 ## 9 Palpatine Human 82 75 170 ## 10 Cliegg Lars Human 82 NA 183 ## # ... with 77 more rows If you specify more than one column, then subsequent columns are used to break ties. Also note that missing values are always displayed last: starwars_short %&gt;% arrange(species, birth_year) ## # A tibble: 87 x 5 ## name species birth_year mass height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Ratts Tyerell Aleena NA 15 79 ## 2 Dexter Jettster Besalisk NA 102 198 ## 3 Ki-Adi-Mundi Cerean 92 82 198 ## 4 Mas Amedda Chagrian NA NA 196 ## 5 Zam Wesell Clawdite NA 55 168 ## 6 IG-88 Droid 15 140 200 ## 7 R2-D2 Droid 33 32 96 ## 8 C-3PO Droid 112 75 167 ## 9 R5-D4 Droid NA 32 97 ## 10 R4-P17 Droid NA NA 96 ## # ... with 77 more rows 3.5.4 mutate() Often you want to add new columns to a data set, e.g. when you calculate new variables or when you want to store re-coded values of other variables. With mutate(), new columns will be added to the end of you data frame. For example, we can resize the height column to provide the body height in m instead of cm. Lets call that variable m_height. Well assign our transformed data (with the newly created m_height column) back into our source object (starwars_short) to keep the changes for the future (and not just print it to the console). starwars_short &lt;- starwars_short %&gt;% # assigns your source object, i.e. data, back to itself to save changes mutate(m_height=height/100) # creates the new variable &quot;m_height&quot; and adds it to the end of the data frame starwars_short # print the data to your console to inspect the new column ## # A tibble: 87 x 6 ## name species birth_year mass height m_height ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Luke Skywalker Human 19 77 172 1.72 ## 2 C-3PO Droid 112 75 167 1.67 ## 3 R2-D2 Droid 33 32 96 0.96 ## 4 Darth Vader Human 41.9 136 202 2.02 ## 5 Leia Organa Human 19 49 150 1.5 ## 6 Owen Lars Human 52 120 178 1.78 ## 7 Beru Whitesun lars Human 47 75 165 1.65 ## 8 R5-D4 Droid NA 32 97 0.97 ## 9 Biggs Darklighter Human 24 84 183 1.83 ## 10 Obi-Wan Kenobi Human 57 77 182 1.82 ## # ... with 77 more rows Lets calculate the BMI of the Star Wars characters with the BMI formula and the newly created m_height variable. Save the changes to your data frame by assigning the source object back to itself. starwars_short &lt;- starwars_short %&gt;% # assigns your source object, i.e. data, back to itself to save changes mutate(BMI= mass/m_height^2) # creates the new variable &quot;BMI&quot; and adds it to the end of the data frame starwars_short # print the data to your console to inspect the new column ## # A tibble: 87 x 7 ## name species birth_year mass height m_height BMI ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Luke Skywalker Human 19 77 172 1.72 26.0 ## 2 C-3PO Droid 112 75 167 1.67 26.9 ## 3 R2-D2 Droid 33 32 96 0.96 34.7 ## 4 Darth Vader Human 41.9 136 202 2.02 33.3 ## 5 Leia Organa Human 19 49 150 1.5 21.8 ## 6 Owen Lars Human 52 120 178 1.78 37.9 ## 7 Beru Whitesun lars Human 47 75 165 1.65 27.5 ## 8 R5-D4 Droid NA 32 97 0.97 34.0 ## 9 Biggs Darklighter Human 24 84 183 1.83 25.1 ## 10 Obi-Wan Kenobi Human 57 77 182 1.82 23.2 ## # ... with 77 more rows mutate() does not merely work with mathematical operators. You can also categorize numeric variables with the case_when function, which is part of the mutate() function. starwars_short &lt;- starwars_short %&gt;% mutate(age_cat= case_when( # &quot;cat&quot; is short for &quot;categorized&quot; birth_year &lt; 20 ~ &quot;very young&quot;, birth_year &lt; 40 ~ &quot;young&quot;, birth_year &lt; 70 ~ &quot;mid-aged&quot;, birth_year &lt;= 100 ~ &quot;old&quot;, birth_year &gt; 100 ~ &quot;very old&quot;) ) starwars_short ## # A tibble: 87 x 8 ## name species birth_year mass height m_height BMI age_cat ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke Skywalker Human 19 77 172 1.72 26.0 very young ## 2 C-3PO Droid 112 75 167 1.67 26.9 very old ## 3 R2-D2 Droid 33 32 96 0.96 34.7 young ## 4 Darth Vader Human 41.9 136 202 2.02 33.3 mid-aged ## 5 Leia Organa Human 19 49 150 1.5 21.8 very young ## 6 Owen Lars Human 52 120 178 1.78 37.9 mid-aged ## 7 Beru Whitesun lars Human 47 75 165 1.65 27.5 mid-aged ## 8 R5-D4 Droid NA 32 97 0.97 34.0 &lt;NA&gt; ## 9 Biggs Darklighter Human 24 84 183 1.83 25.1 young ## 10 Obi-Wan Kenobi Human 57 77 182 1.82 23.2 mid-aged ## # ... with 77 more rows Finally, you can recode variables by using the recode() function, which is part of the mutate() function. Lets be crazy and recode all droids as robots2 and save the result in a new variable called crazy_species! Please note that recode() has an unusual syntax because it follows the order of old_var = new_var instead of the usual order: new_var = old_var. Therefore, recode() is likely to be retired in the future (use case_when instead). starwars_short &lt;- starwars_short %&gt;% mutate(crazy_species=recode( # alternatively, you could also recode directly back into the species variable species, &quot;Droid&quot;=&quot;Robot&quot;)) starwars_short ## # A tibble: 87 x 9 ## name species birth_year mass height m_height BMI age_cat crazy_species ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Skywalker Human 19 77 172 1.72 26.0 very young Human ## 2 C-3PO Droid 112 75 167 1.67 26.9 very old Robot ## 3 R2-D2 Droid 33 32 96 0.96 34.7 young Robot ## 4 Darth Vader Human 41.9 136 202 2.02 33.3 mid-aged Human ## 5 Leia Organa Human 19 49 150 1.5 21.8 very young Human ## 6 Owen Lars Human 52 120 178 1.78 37.9 mid-aged Human ## 7 Beru Whitesun lars Human 47 75 165 1.65 27.5 mid-aged Human ## 8 R5-D4 Droid NA 32 97 0.97 34.0 &lt;NA&gt; Robot ## 9 Biggs Darklighter Human 24 84 183 1.83 25.1 young Human ## 10 Obi-Wan Kenobi Human 57 77 182 1.82 23.2 mid-aged Human ## # ... with 77 more rows Tip for advanced users 1: There is a special case of recoding: Sometimes you will receive data (e.g. through an import from SPSS) in which missings are not marked as NA, but with -9 (or any other number). Unfortunately, you will have to tell R that these are missing values and should be set to NA. In this case, use the na_if() function, which is also part of the mutate() function. Luke Skywalker, the first observation in our data frame, is 172cm tall. For the sake of practice, lets set all heights that are equal to 172cm to NA. This time, we wont save this transformation for later use (by reassigning the source object back to itself) since this transformation does not make a lot of sense. starwars_short %&gt;% mutate(height= na_if(height, 172)) ## # A tibble: 87 x 9 ## name species birth_year mass height m_height BMI age_cat crazy_species ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Skywalker Human 19 77 NA 1.72 26.0 very young Human ## 2 C-3PO Droid 112 75 167 1.67 26.9 very old Robot ## 3 R2-D2 Droid 33 32 96 0.96 34.7 young Robot ## 4 Darth Vader Human 41.9 136 202 2.02 33.3 mid-aged Human ## 5 Leia Organa Human 19 49 150 1.5 21.8 very young Human ## 6 Owen Lars Human 52 120 178 1.78 37.9 mid-aged Human ## 7 Beru Whitesun lars Human 47 75 165 1.65 27.5 mid-aged Human ## 8 R5-D4 Droid NA 32 97 0.97 34.0 &lt;NA&gt; Robot ## 9 Biggs Darklighter Human 24 84 183 1.83 25.1 young Human ## 10 Obi-Wan Kenobi Human 57 77 182 1.82 23.2 mid-aged Human ## # ... with 77 more rows 3.5.5 simmarize() [+ group_by()] Instead of using summarize(), you could omit the American English and write summarise(). This function collapses a data frame into a single row that shows you summary statistics about your variables. Be careful not to overwrite your source object with the collapsed data frame, i.e. do not reassign the source object to itself when you use summarize() (at least unless you have a good reason to do so). starwars_short %&gt;% summarize(mean_height = mean(height, na.rm=TRUE)) # collapses the data frame into one variable called &quot;mean_height&quot; ## # A tibble: 1 x 1 ## mean_height ## &lt;dbl&gt; ## 1 174. # na.rm = TRUE -&gt; removes the missing values prior to the computation of the summary We now know that the average Star Wars character is 174cm tall. But the summarize()function grows especially powerful when it is combined with `group_by to display summary statistics for groups. starwars_short %&gt;% group_by(species) %&gt;% # every unique species becomes its own group summarize(mean_height = mean(height, na.rm=TRUE), # collapses the data frame into one row with one variable called &quot;mean_height&quot;... count = n() # and a second variable that shows the group size (i.e. count) ) ## # A tibble: 38 x 3 ## species mean_height count ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Aleena 79 1 ## 2 Besalisk 198 1 ## 3 Cerean 198 1 ## 4 Chagrian 196 1 ## 5 Clawdite 168 1 ## 6 Droid 131. 6 ## 7 Dug 112 1 ## 8 Ewok 88 1 ## 9 Geonosian 183 1 ## 10 Gungan 209. 3 ## # ... with 28 more rows We learn from the 6 droids in our data set that droids are small, 131cm on average. But Ewoks are even smaller (88cm on average). Pro tip: you can even group by two groups at the same time with the method group_by(x1, x2, .add=TRUE). Finally, we can also retrieve all relevant summary statistics of a classic box plot: starwars_short %&gt;% group_by(species) %&gt;% # every unique species becomes its own group summarize(MAX = max(height, na.rm = TRUE), UQ= quantile(height, 0.75, na.rm = TRUE), M = mean(height, na.rm = TRUE), # SD = sd(height, na.rm = TRUE), # calculating the standard deviation is useless because we often have only 1 observation per species LQ= quantile(height, 0.25, na.rm = TRUE), MIN = min(height, na.rm = TRUE), count = n() # shows the group_size (i.e. count) ) ## # A tibble: 38 x 7 ## species MAX UQ M LQ MIN count ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Aleena 79 79 79 79 79 1 ## 2 Besalisk 198 198 198 198 198 1 ## 3 Cerean 198 198 198 198 198 1 ## 4 Chagrian 196 196 196 196 196 1 ## 5 Clawdite 168 168 168 168 168 1 ## 6 Droid 200 167 131. 96 96 6 ## 7 Dug 112 112 112 112 112 1 ## 8 Ewok 88 88 88 88 88 1 ## 9 Geonosian 183 183 183 183 183 1 ## 10 Gungan 224 215 209. 201 196 3 ## # ... with 28 more rows Tip for advances users 2: If you want to count the unique values of variables, then data %&gt;% group_by(a, b) %&gt;% summarize(n = n()) might not be the best solution (its a lot of code, isnt it?). starwars_short %&gt;% group_by(species, age_cat) %&gt;% summarize(count = n()) ## # A tibble: 51 x 3 ## # Groups: species [38] ## species age_cat count ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Aleena &lt;NA&gt; 1 ## 2 Besalisk &lt;NA&gt; 1 ## 3 Cerean old 1 ## 4 Chagrian &lt;NA&gt; 1 ## 5 Clawdite &lt;NA&gt; 1 ## 6 Droid very old 1 ## 7 Droid very young 1 ## 8 Droid young 1 ## 9 Droid &lt;NA&gt; 3 ## 10 Dug &lt;NA&gt; 1 ## # ... with 41 more rows For more efficient code, you can use the count() function instead: data %&gt;% count(a, b). starwars_short %&gt;% count(species, age_cat) ## # A tibble: 51 x 3 ## species age_cat n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Aleena &lt;NA&gt; 1 ## 2 Besalisk &lt;NA&gt; 1 ## 3 Cerean old 1 ## 4 Chagrian &lt;NA&gt; 1 ## 5 Clawdite &lt;NA&gt; 1 ## 6 Droid very old 1 ## 7 Droid very young 1 ## 8 Droid young 1 ## 9 Droid &lt;NA&gt; 3 ## 10 Dug &lt;NA&gt; 1 ## # ... with 41 more rows 3.5.6 Chaining functions in a pipe All of the dplyr functions can be chained in one single pipe. Using the original starwars_data, well only analyze Star Wars characters who are older than 25 years (filter()), calculate the BMI (mutate()), group them by their species (group_by()) and summarize the average BMI (summarize()). Well display the final result in an ascending order (arrange()). starwars_data %&gt;% mutate(BMI = mass/(height/100)^2) %&gt;% filter(birth_year&gt;25) %&gt;% group_by(species) %&gt;% summarize(mean_BMI = mean(BMI, na.rm = TRUE)) %&gt;% arrange(mean_BMI) ## # A tibble: 14 x 2 ## species mean_BMI ## &lt;chr&gt; &lt;dbl&gt; ## 1 Gungan 17.2 ## 2 Twi&#39;lek 17.4 ## 3 Mirialan 18.8 ## 4 Cerean 20.9 ## 5 Wookiee 21.5 ## 6 Rodian 24.7 ## 7 Human 25.3 ## 8 Mon Calamari 25.6 ## 9 Zabrak 26.1 ## 10 Droid 30.8 ## 11 Trandoshan 31.3 ## 12 Yoda&#39;s species 39.0 ## 13 Hutt 443. ## 14 &lt;NA&gt; NaN 3.6 Take-Aways Tidy data: is a tabular in which each column represents one single variable, each row represents a single observation and each cell contains only one single value Pipe operator: %&gt;% is used to chain functions and apply them to a source object. We call these chains of functions pipes dplyr functions: there are five main dplyr functions that you should know of: select, filter, arrange, mutate, and summarize [+ group_by]. 3.7 Additional tutorials You still have questions? The following tutorials &amp; papers can help you with that: R for Data Science, Chapter 12 R for Data Science, Chapter 5.1.3 and the following The tidyverse style guide Data wrangling with dplyr &amp; tidyr Cheat Sheet To be precise, the pipe operator was introduced to R with the package magrittr, not with dplyr. Nowadays, the %&gt;% operator can be used outside the tidyverse package if magrittr is installed and loaded: library(magrittr). Please, dont hate me for this. "],["tutorial-data-visualization-with-ggplot.html", " 4 Tutorial: Data visualization with ggplot 4.1 Why not stick with Base R? 4.2 Components of a ggplot graph 4.3 Installing &amp; activating ggplot 4.4 Building your first plot 4.5 Other common plot types 4.6 Take Aways 4.7 Additional tutorials", " 4 Tutorial: Data visualization with ggplot After working through Tutorial 4, youll know what each graphical component of a ggplot graph contributes to the final visualization understand the grammer of graphics (or simply: the ggplot2 syntax) to combine graphical components know how to make your own data visualizations using ggplot2 4.1 Why not stick with Base R? The ggplot2 package, i.e. the data visualization package of tidyverse, has become the R package for data visualization. While Base R can be used to visualize data, the ggplot2 package makes data visualization so much easier that I recommend starting with ggplot2 right away and skipping data visualization in Base R altogether. The gg in ggplot2 stands for grammar of graphics, which means that we can describe each component of a graph layer by layer and component by component. You only have to provide ggplot() with a source object (i.e. data) and specify what variables it should map to the aesthetical attributes (color, shape, size) of certain geometric objects (points, lines, bars)  and ggplot will take care of the rest! The inventor of ggplot2, Hadley Wickham, describes the benefits of ggplot2 like this: In order to unlock the full power of ggplot2, youll need to master the underlying grammar. By understanding the grammar, and how its components fit together, you can create a wider range of visualizations, combine multiple sources of data, and customise to your hearts content The grammar makes it easier for you to iteratively update a plot, changing a single feature at a time. The grammar is also useful because it suggests the high-level aspects of a plot that can be changed, giving you a framework to think about graphics, and hopefully shortening the distance from mind to paper. It also encourages the use of graphics customised to a particular problem, rather than relying on specific chart types. (Wickham et al., 2021, no page; bold words inserted) Just as dplyr simplifies data manipulation, ggplot2 simplifies data visualization. In addition, ggplot2 and dplyr work hand in hand: You can prepare your data selection and manipulation with dplyr and pipe it directly into ggplot to turn your transformed data into a beautiful graph. With only a few lines of code, you can produce graphs like this one: This is the code. Right now, it might still look a bit overwhelming to you, but once youve understood the grammar of graphics, it really is a just a small jigsaw puzzle. Moreover, you dont usually start with graphs that are this complicated, but with basic scatter or bar plots. library(ggplot2) plot &lt;- starwars_data %&gt;% filter(species == &quot;Human&quot; | species == &quot;Droid&quot;) %&gt;% ggplot(aes(x = height, y = mass, size = birth_year, fill = species)) + geom_point(shape = 21, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + ggrepel::geom_text_repel(aes(label = name), size = 2.3) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species) To visit the official documentation of ggplot2: - type ?ggplot2 in your console - visit the ggplot documentation - visit the ggplot homepage of the tidyverse 4.2 Components of a ggplot graph As mentioned before, the main idea behind ggplot is to generate a statistical plot by combining layers that represent geometric objects (e.g. points and lines). By linking data to the aesthetic features of these geometric objects (e.g. colors, size, transparency), the aesthetic properties of the geometric objects may be controlled. In the words of Wickham: A graphic maps the data to the aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars). Wickham et al., 2021, no page; bold words inserted Image: The logic of adding layer by layer in ggplot (Source: R @ Ewah 2020): The necessary components of a ggplot graph are: Source object / data: The data that you would like to visualize. Geometries geom_: Geom options allow you to specify what geometric objects will represent the data (i.e. points, bars, lines, and many more). Aesthetics aes(): Aesthetics allows you to map variables to the x- and y-axis and to the aesthetics of those geometric objects (i.e. position, color, size, shape, linetype, and transparency). The complementary, but not necessary components of a ggplot graph are: Scales scale_: Scale options allow you to fine-tune the mapping from the variables to the aesthetics. You can fine-tune axis limits, tick breaks, grid lines, or any other axis/geometric object transformations that depend on the range of a specific scale. Statistical transformations stat_: Allows you to produce statistical summaries of the data for visualization (i.e. means and standard deviations, fitted curves, and many more). Coordinate system coord_: Allows you to change the appearance of your coordinate system (i.e. flip the coordinates to turn horizontal bar chart into a vertical one). Position: to adjust overlapping objects, e.g. jittering, stacking or dodging. Facets facet_: Allows you to divide your plot into multiple subplots. Visual themes theme(): Allows you to specify the visual basics of a plot, such as background, default typeface, sizes, and colors. Axis labels labs(): Allows you to change the plots main title and the axis labels. 4.3 Installing &amp; activating ggplot You can always activate ggplot2 by activating the meta-package tidyverse: library(tidyverse) If for some reason you do not want to activate the whole tidyverse, you should install ggplot2 and activate this package separately: install.packages(&quot;ggplot2&quot;) # install the package (only on the first time) library(ggplot2) # active the package 4.4 Building your first plot In the next sections, you will create your very first plot  layer by layer. We will look at some of the most important components that you will regularly add to graphs and you will learn how to make use of them. 4.4.1 Data Obviously, you need data to perform data visualization. Therefore, our first step is to load the starwars data, but lets keep only humans and droids for now. To this end, assign your transformed data to a new data frame called human_droid_data. human_droid_data &lt;- dplyr::starwars %&gt;% filter(species == &quot;Human&quot; | species == &quot;Droid&quot;) The function ggplot() can only create a plot if we explicitly tell the function what data to use, so this graphical component is necessary. Using our dplyr skills, lets use the human_droid_data as our source object and apply the ggplot() function to it by using a pipe (i.e. %&gt;%). human_droid_data %&gt;% ggplot() The ggplot() function creates a blank canvas (i.e. first layer). We now have to draw on it. 4.4.2 Aesthetics To draw on this blank canvas, we must at least tell the ggplot() function which variables to assign to the x- and y-axis by using the aes() function. Thus, the Aesthetics graph component is also necessary in every single plot. human_droid_data %&gt;% ggplot(aes(x = height, y = mass)) The aes() function allows you to specify the following arguments (and many more, as you will learn over time): x: the variable that should be mapped to the x axis y: the variable that should be mapped to the y size: the variable that should be used for determining the size of a geometric object fill: the variable that should be used for filling a geometric object with a specific color color: the variable that should be used for outlining a geometric object with a specific color 4.4.3 Geometrics Finally, we can turn to the last necessary component of any ggplot graph: the geometric objects that fill your canvas. The choice of these geometric objects determines what kind of chart you create. The geom_ component of the ggplot() function allows you to create the following chart types (and many more, as you will learn over time): geom_bar(): to create a bar chart geom_histogram: to create a histogram geom_line(): to create a line graph geom_point(): to create a scatter or bubble plot geom_boxplot(): to create a box plot Now lets add the data points (x,y) with geom_point() to our canvas to make it a scatter plot: human_droid_data %&gt;% ggplot(aes(x = height, y = mass)) + geom_point() Thats a scatter plot for sure! And you only needed three necessary components to create it: data (i.e. a source object), aesthetics aes(), and geometric objects geom_. 4.4.4 Scales A scale is a mapping from data to the final values that computers can use to actually show the aesthetics. In this sense, a scale regulates the aesthetic mapping of variables to aesthetics. Providing a scale_ is not necessary to create a graph, but it allows you to fine-tune aesthetic mappings to customize your graph. scale_ is very powerful and over time, you will learn about a lot of things that you can customize with it. For now, we will only focus on a few of these. We will use scale_ to : change the limits and ticks of the x and y axis change how a third variable (besides x and y) is mapped to the aesthetics of our geometric object First, we will use scale_ to modify the x and the y axis by providing the graph with new axis limits. human_droid_data %&gt;% ggplot(aes(x = height, y = mass)) + geom_point() + scale_y_continuous(limits = c(30, 140)) + # modify the y axis limits scale_x_continuous(limits = c(90, 210)) # modify the x axis limits Second, well add more ticks to make the graph better readable. human_droid_data %&gt;% ggplot(aes(x = height, y = mass)) + geom_point() + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + # choose where the ticks of the y axis appear scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) # choose where the ticks of the x axis appear Until now, we have used scale_ to transform only the axes. But we can also use it to change the mapping of variables to geometric objects. To demonstrate this, we now add another variable to our graph, namely the age (birth_year) of the humanoid and droid Star Wars characters. Lets map age to our data points (i.e. geom_point()) so that larger bubbles reflect older age. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + # map birth_year (age) to the size of the following geometric objects geom_point() + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) Personally, I feel like these bubbles could use a little bit of rescaling to make age differences stand out more. In addition, you could get a nicer title for the size legend than birth_year. Lets try that. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point() + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) # sets the bubbles&#39; size in a range between 1 and 11 and renames the respective legend title to &quot;age&quot; Perfect! The legend spells age and age differences seem a bit more obvious now. Unfortunately, some data points are now overlapping. I think this is a good time to introduce you to the differences between using scale_ and adding aesthetics to the geom_ objects directly. While the former allows you to change the mapping from variables to the aesthetics of geometric objects, the latter one allows you to provide a constant. This means that the aesthetic mapping does not depend on the values of a variable, but is set to a single default value. To demonstrate this and fix the overlap of our bubbles, we change the transparency value of the bubbles so that they become transparent. Note that all the values given are constants, which means that they do not depend on a third variable like birth_year. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + # shape = 21 is creating bubbles that have a border (i.e. outline), fill = &quot;black&quot; fills the bubble with black ink, alpha = 0.25 to make the bubbles` black ink 25% transparent and color = &quot;black&quot; to make the border (i.e. outline) pitch black scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) This looks way more readable. I think we are ready to move on to Themes. 4.4.5 Themes Just like scale_, theme_is an optional ggplot component, i.e. not necessary. Themes are visually appealing presets for charts, e.g., they influence whether grid lines are visible or whether certain color palettes are applied to the data. By using themes you can make your graphs more beautiful and give them a consistent style without any effort, which is especially useful for longer texts like theses. To familiarize yourself with the various options, take a look at this overview of all ggplot2 themes. If you dont like grid lines, for example, theme_classic() might be to your taste: human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_classic() Personally, I really enjoy the theme_bw() (black-and-white theme). So lets apply it to our graph: human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() 4.4.6 Labs Again, labs() is not a necessary, but an optional component of your graph. Using the labs() function allows you to set a main title for your plot and to change the labels of the x and y axis. Lets try it: human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) Now that weve added a main title, it becomes clear that we cant really distinguish the data points that represent humans from those that represent droids. 4.4.7 Facets Faceting divides plot into tiny subplots, which display different subsets of your data. Facets are an effective way to explore your data because they allow you to rapidly detect divergent patterns in these subsets. Of course, faceting is optional, i.e. not necessary. You dont need faceting if you dont want to compare different groups within your data. The two approaches to faceting are: facet_wrap(): uses the levels of one (or more) variable(s) to create groups + panels for each group; useful if you have a single categorical variable with many levels facet_grid(): produces a matrix of panels defined by two variables which form the rows and columns Image: The logic of faceting (Source: Wickham et al., 2021): Lets use the facet_wrap() function to create two subplots for our two different levels of the species variable: Droid and Human. You can provide two arguments to facet_wrap(): ~, followed by the grouping variable nrow: the number of rows in which panels should be placed human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species, nrow=2) # using ~grouping_variable and nrow = 2 shows the two panels on top of each other Great, finally we can distinguish the data points representing humans from those representing droids! On the left side, however, the panel with the humans looks a bit empty. Maybe we should put them next to each other. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year)) + geom_point(shape = 21, fill = &quot;black&quot;, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species) # nrow=1 is the default, so you don´t have to call it explicitly I like that! 4.4.8 Saving graphs I think its time that we save this plot. To finish of this masterpiece (and make it less triste), lets add some final colors before saving. Well fill our bubbles with colorful ink based on the species variable, so we need to add fill=species to the aes() and remove the default black ink provided in the geom_point() function. human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year, fill=species)) + geom_point(shape = 21, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species) Congratulations! You have just managed to recreate the plot from the beginning of this tutorial! The only thing we havent covered yet is the labeling of all data points, because for that youd need the ggrepel package to not mess up the labels - and thats not part of ggplot2. So lets skip that and save our graph. To use your graph in another document, e.g. a theses written in Word, youll have to export the plot first. Therefore, you must assign your plot to a new object and call the ggsave() function on that object. The plot will be saved to your working directory and formatted according to the file extension you specified (for example: .jpeg or .png). plot &lt;- human_droid_data %&gt;% ggplot(aes(x = height, y = mass, size=birth_year, fill=species)) + geom_point(shape = 21, alpha = 0.25, color = &quot;black&quot;) + scale_y_continuous(limits = c(30, 140)) + scale_x_continuous(limits = c(90, 210)) + scale_y_continuous(breaks = c(40, 60, 80, 100, 120, 140, 160)) + scale_x_continuous(breaks = c(100, 120, 140, 160, 180, 200)) + scale_size(range = c(1, 11), name = &quot;age&quot;) + theme_bw() + labs(title = &quot;Mass vs. height of humans and droids in Star Wars&quot;, x = &quot;Height (cm)&quot;, y = &quot;Weight (kg)&quot;) + facet_wrap(~species) ggsave(filename = &quot;mass_vs_height.jpeg&quot;, plot) 4.5 Other common plot types I cant give an overview of all possible types of plots, but I can at least touch a bit on how other common types of geom_ behave. 4.5.1 bar plots Bar plots are very common. They are either (1) used to display the frequency with which a certain factor level of a categorical variable occurs or (2) to display relationships between a categorical variable and a metric variable. So lets create a quick bar plot using the sex variable (categorical, three factor levels) and get an overview on how many human and droidic Star Wars characters are male, female, or do not have a sex. human_droid_data %&gt;% ggplot(aes(x = sex)) + # We only have to specify the variable that we want to get the count for (i.e. number of observations) geom_bar() Next, lets look at the relationship between sex and the height (metric) variable. We will produce a bar plot that displays the mean height of each group: human_droid_data %&gt;% ggplot(aes(x = sex, y=height)) + # Now we need to specify the variable that we want to summarize with mean statistics geom_bar(stat = &quot;summary&quot;, fun.y = &quot;mean&quot;) # apply the summary statistic of y (mean) to the geom_bars Maybe we want to sort the bars according to their mean. Lets reorder the factor levels manually. human_droid_data %&gt;% mutate(sex = factor(sex, levels = c(&quot;male&quot;, &quot;female&quot;, &quot;none&quot;))) %&gt;% ggplot(aes(x = sex, y=height)) + stat_summary(geom = &quot;bar&quot;, fun = &quot;mean&quot;) And if we would like to have a horizontal bar plot, we can use the coord_ component of ggplot() to flip the coordinates. human_droid_data %&gt;% mutate(sex = factor(sex, levels = c(&quot;male&quot;, &quot;female&quot;, &quot;none&quot;))) %&gt;% ggplot(aes(x = sex, y=height)) + stat_summary(geom = &quot;bar&quot;, fun = &quot;mean&quot;) + coord_flip() 4.5.2 box plots Box plots are a great option to summarize metric variables (by groups). They provide you with the Five-number-summary: the sample minimum (smallest observation)  lower whisker the lower quartile  lower end of the box the median (the middle value)  thick black line the upper quartile  upper end of the box the sample maximum (largest observation)  upper whisker Lets create box plots of the height for human and droidic Star Wars characters who are male, female, or do not have a sex. human_droid_data %&gt;% ggplot(aes(x = sex, y=height)) + geom_boxplot() 4.6 Take Aways graph creation: ggplot() mapping variables to aesthetics: aes(x, y, color, fill, size, etc.) chart type: geom_bar(), geom_line(), geom_point(), geom_boxplot() (for example) titles: labs() axis limits/ticks: scale_x_continuous(), scale_y_continuous() mapping variables to geom size: scale_size() themes: theme_classic(), theme_light(), theme_bw() (for example) faceting: facet_wrap() or facet_grid save images: ggsave() 4.7 Additional tutorials You still have questions? The following tutorials &amp; papers can help you with that: Chang, W. R (2021). R Graphics Codebook. Practical Recipes for Visualizing Data. Link Wickham, H., Navarro, D., &amp; Pedersen, T. L. (2021). ggplot2: elegant graphics for data analysis. Online, work-in-progress version of the 3rd edition. Link Hehman, E., &amp; Xie, S. Y. (2021). Doing Better Data Visualization. Advances in Methods and Practices in Psychological Science. DOI: 10.1177/25152459211045334 Link R Codebook by J.D. Long and P. Teetor, Tutorial 10 "],["text-data-manipulation-with-stringr.html", " 5 Text data manipulation with stringr 5.1 Whats stringr? 5.2 Working with strings 5.3 Working with string patterns 5.4 Working with regular expressions 5.5 Take-Aways 5.6 Additional tutorials", " 5 Text data manipulation with stringr After working through Tutorial 5, youll understand the concept of string patterns and regular expressions know how to search for string patterns 5.1 Whats stringr? The stringr package is another package of the tidyverse family, i.e., it comes pre-installed with the tidyverse. The package offers a neat set of functions that makes working with strings really simple for beginners. Therefore, stringr is a good place to start getting into text data management. A string is a data type that is used to represent text rather than numbers. 5.2 Working with strings First, lets create a vector that contains some strings and print the vector to the console! fruits &lt;- c(&quot;banana&quot;, &quot;apple&quot;, &quot;pear&quot;, &quot;strawberry&quot;, &quot;raspberry&quot;, &quot;kiwi&quot;) fruits ## [1] &quot;banana&quot; &quot;apple&quot; &quot;pear&quot; &quot;strawberry&quot; &quot;raspberry&quot; &quot;kiwi&quot; First, we want to know how long each of these strings is, i.e., how many characters the elements of the fruits vector contain. We will use the str_length() function. str_length(fruits) ## [1] 6 5 4 10 9 4 That was easy. Next, we want to join multiple strings into a single string. We will use the str_c function. str_c(fruits, collapse = &quot; and &quot;) ## [1] &quot;banana and apple and pear and strawberry and raspberry and kiwi&quot; # If collapse is not NULL, it will be inserted between elements of the result, here: and You can also change the order of the str_c function: str_c(&quot;My favourite fruit is: &quot;, fruits, collapse = NULL) ## [1] &quot;My favourite fruit is: banana&quot; &quot;My favourite fruit is: apple&quot; &quot;My favourite fruit is: pear&quot; ## [4] &quot;My favourite fruit is: strawberry&quot; &quot;My favourite fruit is: raspberry&quot; &quot;My favourite fruit is: kiwi&quot; Lets say, we want to extract substrings from a character vector. For example, we only want to keep the second to fourth letter of each string. We can use the str_sub function to achieve that. str_sub(fruits, start = 2, end = 4) ## [1] &quot;ana&quot; &quot;ppl&quot; &quot;ear&quot; &quot;tra&quot; &quot;asp&quot; &quot;iwi&quot; 5.3 Working with string patterns Often we want to search for certain string patterns in a text document. String patterns are character sequences (for instance, letter, numbers, or special characters). Lets assume for a second that we have misspelled one of our fruits (banana, we have switched the na letters to an). misspelled_fruits &lt;- c(&quot;baanan&quot;, &quot;apple&quot;, &quot;pear&quot;, &quot;strawberry&quot;, &quot;raspberry&quot;, &quot;kiwi&quot;) It would be really great if we could search for the string pattern an and replace it with the string pattern na automatically, wouldnt it? Well, stringroffers some functions to do just that. For example, str_detect() tells you if theres any match to the pattern. str_detect(misspelled_fruits, &quot;an&quot;) ## [1] TRUE FALSE FALSE FALSE FALSE FALSE Now we know that there is one word that contain letters that match the an pattern. We have a misspelling! What is that word? str_subset() extracts the matching strings, so that we can find out. str_subset(misspelled_fruits, &quot;an&quot;) ## [1] &quot;baanan&quot; Of course, its banana. But how many times has an been misspelled in banana? Just once? Lets find out with str_count(, which counts the number of patterns in each string. str_count(misspelled_fruits, &quot;an&quot;) ## [1] 2 0 0 0 0 0 Two times! Lets fix that with the str_replace function. misspelled_fruits &lt;- str_replace(misspelled_fruits, &quot;anan&quot;, &quot;nana&quot;) misspelled_fruits ## [1] &quot;banana&quot; &quot;apple&quot; &quot;pear&quot; &quot;strawberry&quot; &quot;raspberry&quot; &quot;kiwi&quot; Perfect! We have fixed our misspelled fruits. However, keep in mind that pattern correction can mess up your string pretty badly if you are not cautious. Therefore, you should always explore your strings very thoroughly before replacing any string patterns. For example, lets see what our pattern detection will uncover if our misspelled fruits would contain an additional orange, which has been spelled correctly: misspelled_fruits &lt;- c(&quot;baanan&quot;, &quot;apple&quot;, &quot;pear&quot;, &quot;strawberry&quot;, &quot;raspberry&quot;, &quot;kiwi&quot;, &quot;orange&quot;) str_detect(misspelled_fruits, &quot;an&quot;) ## [1] TRUE FALSE FALSE FALSE FALSE FALSE TRUE Now, str_detect() matches two words with the an pattern, but the latter is not a misspelling! So always be careful! As a final lesson, you can also split a string into multiple strings based on certain string patterns using the str_split()function: csv_fruits &lt;- c(&quot;banana, apple, pear, strawberry, raspberry, kiwi, orange&quot;) str_split(csv_fruits, &quot;,&quot;) ## [[1]] ## [1] &quot;banana&quot; &quot; apple&quot; &quot; pear&quot; &quot; strawberry&quot; &quot; raspberry&quot; &quot; kiwi&quot; &quot; orange&quot; 5.4 Working with regular expressions Often, we want to match more complicated string patterns than a simple an. For example, we might wish to detect all strings in our text document that do not start with RT , because RT at the beginning of a string implies a retweet rather than an original tweet when analyzing Twitter data. Arguably, we are often not really interested in analyzing retweets (but sometimes we are, it depends on the research question). To search and match complex string patterns, we need regular expressions. Regular expressions (short: regex) are a concise language for describing patterns of text. Regex should not be taken literally, but have a non-literal meaning. Lets keep working with our (non-misspelled) fruits vector to display what regex can do. First, lets look for all strings that start with the letter b using the ^ (start of string) regex. str_detect(fruits, &quot;^b&quot;) # ^ stands for &quot;start of string&quot;, i.e. we are matching for string that start with the letter b ## [1] TRUE FALSE FALSE FALSE FALSE FALSE Thats on point because only our first entry, banana, starts with b and str_detect() macthed that correctly! Please, note the difference to not matching the regex, but the simple string pattern b. str_detect(fruits, &quot;b&quot;) # matches all strings that contain the letter b at any place ## [1] TRUE FALSE FALSE TRUE TRUE FALSE This has matched banana, strawberry and raspberry, because all of these fruits contain a letter b at some place. Finally, you should also not confuse ^b with [^b], because [^] stands for anything but in regex language. fruits &lt;- c(&quot;banana&quot;, &quot;apple&quot;, &quot;pear&quot;, &quot;strawberry&quot;, &quot;raspberry&quot;, &quot;kiwi&quot;, &quot;b&quot;, &quot;bbb&quot;) str_detect(fruits, &quot;[^b]&quot;) # matches all strings that contain any letters different from b(s) ## [1] TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE Next, lets say that it doesnt really matter to use whether our fruits contains the letter b or the letter e at any position of the string. There is a very powerful regex to match this string pattern: the match any one of operator []. fruits &lt;- c(&quot;banana&quot;, &quot;apple&quot;, &quot;pear&quot;, &quot;strawberry&quot;, &quot;raspberry&quot;, &quot;kiwi&quot;) str_detect(fruits, &quot;[be]&quot;) # matches all strings that contain either the letter b or the letter e ## [1] TRUE TRUE TRUE TRUE TRUE FALSE Our regex has managed to match all strings that contain either the letter b or the letter e, which leaves only kiwi to be FALSE. Next, we can match all fruits that contain letters that range between s to w in the ABC. We will need to use the range operator [-]: str_detect(fruits, &quot;[s-w]&quot;) # matches all strings that contain either the letter s, t, u, v, or, w ## [1] FALSE FALSE FALSE TRUE TRUE TRUE Again, we have a perfect match! The str_detect has successfully matched strawberry, raspberry, and kiwi. Next, we might want to match all fruits that contain more than one r, i.e., we want to match strawberry and raspberry, but not pear. This is where the ? operator (zero or one) * operator (zero or more), the + operator (one or more), and the {n} operator (exactly n) come in handy. str_detect(fruits, &quot;r?&quot;) # matches all strings that contain zero or one rs ## [1] TRUE TRUE TRUE TRUE TRUE TRUE str_detect(fruits, &quot;r*&quot;) # matches all strings that contain zero or more rs ## [1] TRUE TRUE TRUE TRUE TRUE TRUE str_detect(fruits, &quot;r+&quot;) # matches all strings that contain one or more rs --&gt; this grabs pear as well, not there yet! ## [1] FALSE FALSE TRUE TRUE TRUE FALSE str_detect(fruits, &quot;r{2}&quot;) # matches all strings that contain exactly two rs --&gt; this grabs only the berries, yeah! ## [1] FALSE FALSE FALSE TRUE TRUE FALSE This looks great! We now know the most important regular expressions! If you feel like you need even more advanced regular expressions, you can look them up in this awesome stringr cheat sheet. 5.5 Take-Aways String patterns &amp; RegEx: String patterns are sequences of characters; regular expressions are a type of string patterna used to match or detect other string patterns in texts. Important regular expressions: The best overview of all regex options can be found on the stringr cheat sheet. 5.6 Additional tutorials You still have questions? The following tutorials, books, &amp; tools may help you: Stringr Cheat Sheet Regexr - Learn, build, and test RegEx Text as Data by V. Hase, Tutorial 9 "],["tidy-text-analysis.html", " 6 Tidy text analysis 6.1 What is tidy text? 6.2 Example 6.3 Topic modeling 6.4 Take-Aways 6.5 Additional tutorials", " 6 Tidy text analysis After working through Tutorial 6, youll understand the concept of tidy text know how to combine tidy text management approaches with regular expressions be able to produce first analyses, e.g., word frequencies 6.1 What is tidy text? Since youve already learnt what tidy data is (see Tidy data), you can make an educated guess about the meaning of tidy text! In contrast to the ways text is commonly stored in existing text analysis approaches (e.g., as strings or document-term matrices), the tidy text format is a table with one single token per row (token = the unit of analysis). A token is a meaningful unit of text, i.e., a word, a sentence, or a paragraph that we are interested in using for further analysis. Splitting text into tokens is what we call the process of tokenization. Julia Silge and David Robinsons tidytext package makes tokenizing into the tidy text format simple! Moreover, the tidytext package builds upon tidyverse and ggplot2, which makes it easy to use for you, since you already know these packages (see Tutorial: Data management with tidyverse and Tutorial: Data visualization with ggplot for a recap). Thats why well focus on the tidytext in this breakout session. This tutorial is based on Julia Silge and David Robinsons open-access book Text Mining in R. A Tidy Approach and a lot of the following code was actually written by Julia Silge. If you want to dig deeper into tidy text analysis, you should check the book out. Both authors have also created an informative flowchart of the tidy text analysis workflow: Image: Tidy Text Analysis Workflow 6.2 Example 6.2.1 First steps To give you an example of what the tidytext package is capable of, lets create some toy data first. Look at this tweet about the Russian aggression against Ukraine and the Eurovision Song Contest: Image: Tweet about the Russian Aggression Against Ukraine and the Eurovision Song Contest Lets save the text of this tweet into a variable called tweet, which will consist of 8 separate strings for each individual clause of the tweet. tweet &lt;- c(&quot;To the ppl not getting it &quot;, &quot;it&#39;s about showing solidarity.&quot;, &quot;That&#39;s why the majority voted for Ukraine.&quot;, &quot;It&#39;s not going to stop the war &quot;, &quot;but it&#39;s another gesture to show Europe stands with them.&quot;, &quot;Some of these men might not be here next year.&quot;, &quot;#Eurovision&quot;, &quot;#esc2022&quot;) tweet ## [1] &quot;To the ppl not getting it &quot; ## [2] &quot;it&#39;s about showing solidarity.&quot; ## [3] &quot;That&#39;s why the majority voted for Ukraine.&quot; ## [4] &quot;It&#39;s not going to stop the war &quot; ## [5] &quot;but it&#39;s another gesture to show Europe stands with them.&quot; ## [6] &quot;Some of these men might not be here next year.&quot; ## [7] &quot;#Eurovision&quot; ## [8] &quot;#esc2022&quot; Next, install and load the tidytext package, the tidyverse package &amp; the lubridate package. lubridate will convert dates to a standard format, making it easier to create graphs using time data. # installing/loading the package: if(!require(tidytext)) { install.packages(&quot;tidytext&quot;); require(tidytext) } #load / install+load tidytext # installing/loading lubridate: if(!require(lubridate)) { install.packages(&quot;lubridate&quot;); require(lubridate) } #load / install+load lubridate library(lubridate) library(tidytext) library(tidyverse) Now, we need to turn our tweet variable into a table using the tibble() function, which is part of the tibble package that comes pre-installed with tidyverse. tweet_df &lt;- tibble(line = 1:length(tweet), # this line creates a column called &#39;line&#39; that contains the row number, starting with 1 and ending with the last row text = tweet) # this line will create a column called &#39;text&#39; that contains all text information from our tweet variable tweet_df ## # A tibble: 8 x 2 ## line text ## &lt;int&gt; &lt;chr&gt; ## 1 1 &quot;To the ppl not getting it &quot; ## 2 2 &quot;it&#39;s about showing solidarity.&quot; ## 3 3 &quot;That&#39;s why the majority voted for Ukraine.&quot; ## 4 4 &quot;It&#39;s not going to stop the war &quot; ## 5 5 &quot;but it&#39;s another gesture to show Europe stands with them.&quot; ## 6 6 &quot;Some of these men might not be here next year.&quot; ## 7 7 &quot;#Eurovision&quot; ## 8 8 &quot;#esc2022&quot; Our tweet is now displayed as a table with 8 rows and one variable that contains the actual text information. If our unit of analysis, i.e. token, is a single word, then this data frame cannot be considered tidy because it does not follow the one-token-per-row logic. Breaking the text into separate tokens (tokenization) and transforming it into a clean data structure is required. We are in luck: The tidytext package provides the unnest tokens() function to do just that. tweet_df_tknzd &lt;- tweet_df %&gt;% unnest_tokens(word, text) # this line takes the column called &#39;text&#39; from the tweet_df and tokenizes (unit of analysis: word) its content into a new column called &#39;word&#39; tweet_df_tknzd ## # A tibble: 46 x 2 ## line word ## &lt;int&gt; &lt;chr&gt; ## 1 1 to ## 2 1 the ## 3 1 ppl ## 4 1 not ## 5 1 getting ## 6 1 it ## 7 2 it&#39;s ## 8 2 about ## 9 2 showing ## 10 2 solidarity ## # ... with 36 more rows Note: unnest tokens() comes with a few handy preprocessing festures, i.e., it already turns the text into a standardized format for furtehr analysis: 1. Line numbers from which each word originated are kept in the data frame (column: line). 2. All punctuation has already been taken care of, i.e., dots, questions marks, etc. have been removed. 3. All uppercase letters habe veen taken care of, i.e., words have been transformed to lowercase.3 6.2.2 Preprocessing real-world data Now that you have seen what the tidytext package is capable of, lets try it on some real-world data. Ive provided you with a data file that contains data collected with the Twitter API about the Russian aggression against Ukraine. You can download this file on my Github page: Download here. Lets import the data into RStudio and inspect it using the View() function. library(openxlsx) setwd(&quot;/Users/LaraK/Documents/Hackathon22&quot;) data &lt;- read.csv(&quot;ukraine_tweets2.csv&quot;, encoding = &quot;UTF-8&quot;) Sys.setenv(TZ=&quot;UTC&quot;) # Twitter gives a UTC timestamp, so we&#39;ll need to set our own environment to UTC, too data %&gt;% mutate(time = lubridate::ymd_hms(time)) %&gt;% # turn the data column into a proper format with the lubridate package (this helps to create time series graphs!) tibble() ## # A tibble: 53,097 x 18 ## time user user.id user.location user.description user.followers_~ hashtag0 hastag1 ## &lt;dttm&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2022-02-25 18:10:59 peekay14 6.34e 7 &quot;&quot; &quot;&quot; 30 &lt;NA&gt; &lt;NA&gt; ## 2 2022-02-25 18:10:59 SocialistAct 2.34e 8 &quot;Britain&quot; &quot;Twitter accoun~ 17583 &lt;NA&gt; &lt;NA&gt; ## 3 2022-02-25 18:10:59 Terry93715450 1.07e18 &quot;&quot; &quot;&quot; 6 &lt;NA&gt; &lt;NA&gt; ## 4 2022-02-25 18:10:59 sodronen 1.19e 8 &quot;Bergen, Norge&quot; &quot;science-policy~ 727 war Ukraine ## 5 2022-02-25 18:10:59 jab611110 1.95e 9 &quot;&quot; &quot;&quot; 10 &lt;NA&gt; &lt;NA&gt; ## 6 2022-02-25 18:10:59 PhilSheaS 5.42e 7 &quot;St Albans or bik~ &quot;Dad x2, husban~ 629 &lt;NA&gt; &lt;NA&gt; ## 7 2022-02-25 18:10:59 JAMBurgoyne 1.04e18 &quot;Wolverhampton, E~ &quot;\\U0001f3f4\\U00~ 791 &lt;NA&gt; &lt;NA&gt; ## 8 2022-02-25 18:10:59 SarahBCalif 7.50e17 &quot;SoCal &quot; &quot;| Mom | Wife |~ 39808 &lt;NA&gt; &lt;NA&gt; ## 9 2022-02-25 18:10:59 halonorsu 1.28e18 &quot;&quot; &quot;Hope 2022 is o~ 4 peace Ukraine ## 10 2022-02-25 18:10:59 zabi7474 1.16e 8 &quot;Lahore, Pakistan&quot; &quot;Digital Journa~ 101 &lt;NA&gt; &lt;NA&gt; ## # ... with 53,087 more rows, and 10 more variables: mention0.at_name &lt;chr&gt;, mention0.screen_name &lt;chr&gt;, ## # mention0.id &lt;dbl&gt;, reply_to &lt;chr&gt;, reply_to_id &lt;dbl&gt;, full_text &lt;chr&gt;, id_str &lt;dbl&gt;, is_quote_status &lt;lgl&gt;, ## # favorite_count &lt;int&gt;, retweet_count &lt;int&gt; View(data) This data set contains a lot of information! We get to know who wrote the tweet (user and the unique user.id) and where this person lives (user.location). But most importantly, we can find the text of the tweet in the column full_text. First of all, lets reduce some of these columns that we dont need for our text analysis. data_short &lt;- data %&gt;% select(time, user, full_text) # keeps time, user, and full_text View(data_short) Now, to get an overview, lets create a visualization of the users who posted at least 18 tweets. data_short %&gt;% group_by(user) %&gt;% summarize(n = n()) %&gt;% filter(n &gt; 18) %&gt;% ggplot(aes(x = user, y = n)) + stat_summary(geom = &quot;bar&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 90, vjust = 0.00, hjust = 0.00)) + labs(title = &quot;Users who posted the most tweets about the Russian\\naggression against Ukraine&quot;, x = &quot;User name&quot;, y = &quot;Number of Tweets&quot;) Now that we have had our overview, we will start with a process called text normalization. Text normalization is the endeavor to minimize the variability of a text by bringing it closer to a specified standard by reducing the quantity of divergent data that the computer has to deal with. Therefore, text normalization boots efficiency. Some techniques of text normalization will be covered in the next few section, for example, tokenization, stop word removal, and stemming/lemmatization. 6.2.2.1 Tokenization I think we are ready to tokenize the data with the unnest tokens() function. Tokenization is the process of breaking text into words (and punctuation marks). Since scientists are almost never interested in the punctuation marks and will delete them from the data set later anyway, unnest tokens() comes with the nice bonus of deleting the punctuation marks directly, leaving only words as tokens. We will use the highly specialized token = \"tweets\" option of the tidytext package (by Mullen 2016), which preserves hashtags and mentions of users with the @ sign from the direct punctuation removal process performed by unnest tokens(). Another common practice is to remove all tokens that contain numbers. Depending on the research question, it can also be helpful to remove URLs. For illustrative purposes, we will do both. But keep in mind that removing all URLs will take the option from you to compare what accounts have shared certain URLs more frequently than others. remove_reg &lt;- &quot;&amp;amp;|&amp;lt;|&amp;gt;&quot; # &amp;amp; = &amp; in HTML, &amp;lt; = &lt; in HTML, &amp;gt; = &gt; in HTML data_tknzd &lt;- data_short %&gt;% mutate(tweet = row_number()) %&gt;% filter(!str_detect(full_text, &quot;^RT&quot;)) %&gt;% # removes all retweets, i.e., tweets that are not unique but a reposts mutate(text = str_remove_all(full_text, remove_reg)) %&gt;% # remove special HTML characters (&amp;, &lt;, &gt;) unnest_tokens(word, full_text, token = &quot;tweets&quot;) %&gt;% # using `to_lower = TRUE` with `token = &#39;tweets&#39;` can break URLs, so be sure that you don&#39;t need them filter(!str_detect(word, &quot;^[:digit:]+$&quot;)) %&gt;% # remove all words that are numbers, e.g. &quot;2020&quot; filter(!str_detect(word, &quot;^http&quot;)) # remove all words that are a URL, e.g., (https://evoldyn.gitlab.io/evomics-2018/ref-sheets/R_strings.pdf) head(data_tknzd) ## time user tweet ## 1 2022-02-25 18:10:59 peekay14 1 ## 2 2022-02-25 18:10:59 peekay14 1 ## 3 2022-02-25 18:10:59 peekay14 1 ## 4 2022-02-25 18:10:59 peekay14 1 ## 5 2022-02-25 18:10:59 peekay14 1 ## 6 2022-02-25 18:10:59 peekay14 1 ## text ## 1 @vonderleyen @EU_Commission rubbish! Your words and those of your fellow EU politicians are baseless and useless to the people of Ukraine! Do something to help them, more severe sanctions against Russia or send in troops, but stop talking and do something, innocent people are being attacked in Europe!! ## 2 @vonderleyen @EU_Commission rubbish! Your words and those of your fellow EU politicians are baseless and useless to the people of Ukraine! Do something to help them, more severe sanctions against Russia or send in troops, but stop talking and do something, innocent people are being attacked in Europe!! ## 3 @vonderleyen @EU_Commission rubbish! Your words and those of your fellow EU politicians are baseless and useless to the people of Ukraine! Do something to help them, more severe sanctions against Russia or send in troops, but stop talking and do something, innocent people are being attacked in Europe!! ## 4 @vonderleyen @EU_Commission rubbish! Your words and those of your fellow EU politicians are baseless and useless to the people of Ukraine! Do something to help them, more severe sanctions against Russia or send in troops, but stop talking and do something, innocent people are being attacked in Europe!! ## 5 @vonderleyen @EU_Commission rubbish! Your words and those of your fellow EU politicians are baseless and useless to the people of Ukraine! Do something to help them, more severe sanctions against Russia or send in troops, but stop talking and do something, innocent people are being attacked in Europe!! ## 6 @vonderleyen @EU_Commission rubbish! Your words and those of your fellow EU politicians are baseless and useless to the people of Ukraine! Do something to help them, more severe sanctions against Russia or send in troops, but stop talking and do something, innocent people are being attacked in Europe!! ## word ## 1 @vonderleyen ## 2 @eucommission ## 3 rubbish ## 4 your ## 5 words ## 6 and As you can see, unnest tokens() in combination with the token = \"tweets\" option has kept all mentions as tokens. Add %&gt;% filter(!str_detect(word, \"^@\")) at the end of the above presented code if you want to remove these, too. Now, lets check the total number of unique words in our tweet data: paste(&quot;We are investigating &quot;, dim(data_tknzd)[1], &quot; non-unique features / words and &quot;, length(unique(data_tknzd$word)), &quot; unique features / words.&quot;) ## [1] &quot;We are investigating 250371 non-unique features / words and 23614 unique features / words.&quot; 6.2.2.2 Removing stop words Next, we should get rid of stop words. Stop words are a group of words that are regularly used in a language. In English, stop words such as the, is, and and would qualify. Because stop words are so common, they dont really tell us anything about the content of a tweet and what differentiates this tweet from other tweets. The tidytext package comes with a pre-installed stop word data set. Lets save that data set to a source object called stop_word_data. This way, we can use it later. stop_word_data &lt;- tidytext::stop_words head(stop_word_data, 20) # prints the first 20 stop words to the console ## # A tibble: 20 x 2 ## word lexicon ## &lt;chr&gt; &lt;chr&gt; ## 1 a SMART ## 2 a&#39;s SMART ## 3 able SMART ## 4 about SMART ## 5 above SMART ## 6 according SMART ## 7 accordingly SMART ## 8 across SMART ## 9 actually SMART ## 10 after SMART ## 11 afterwards SMART ## 12 again SMART ## 13 against SMART ## 14 ain&#39;t SMART ## 15 all SMART ## 16 allow SMART ## 17 allows SMART ## 18 almost SMART ## 19 alone SMART ## 20 along SMART Now, lets use these stop words to remove all tokens that are stop words from our tweet data. data_tknzd &lt;- data_tknzd %&gt;% filter(!word %in% stop_word_data$word) %&gt;% # removes all of our tokens that are stop words filter(!word %in% str_remove_all(stop_word_data$word, &quot;&#39;&quot;)) # first removes all &#39; from the stop words (e.g., ain&#39;t -&gt; aint) and then removes all of our tokens that resemble these stop words without punctuation 6.2.2.3 Lemmatizing &amp; stemming Minimizing words to their basic form (lemmatizing) or root (stemming) is a typical strategy of reducing the number of words in a text. Stemming: A stem is the root form of a word without any suffixes. A stemming algorithm (= stemmer) removes the suffixes and returns the stem of the word. Example1: vengeance &gt; vengeanc, or, if you use a very aggressive stemmer, vengeance &gt; veng || Example2: legions &gt; legion, or, if you use a very aggressive stemmer, legions &gt; leg (this one is problematic!) || Example3: murdered &gt; murder Lemmatization: A lemma is a lexicon headword or, i.e., the dictionary-matched basic form of a word (as opposed to a stem created by eliminating or changing suffixes). Because a lemmatizing algorithm (= lemmatizer) performs dictionary matching, lemmatization is more computationally demanding than stemming. vengeance &gt; vengeance || Example2: legions &gt; legion || Example3: murdered &gt; murdered (this one is problematic!) Most of the time, stemmers will make a lot of mistakes (e.g., legions &gt; leg) and lemmatizers will make fewer mistakes. However, lemmatizers summarize fewer words (murdered &gt; murdered) and therefore reduce the word count less efficiently than stemmers. Which technique of text normalization is preferred is always determined by the research question and the available data. For the ease of teaching, we will use stemming on our tweet data. However, you could also use lemmatization with the spacyr package. Well use Porters (1980) stemming algorthm, which is the most extensively used stemmer for the English language. Porter made the stemmer open-source, and you may use it with R using the SnowballC package (Bouchet-Valat 2020). # installing/loading SnowballC: if(!require(SnowballC)) { install.packages(&quot;SnowballC&quot;); require(SnowballC) } #load / install+load SnowballC data_tknzd &lt;- data_tknzd %&gt;% mutate(word = wordStem(word)) head(data_tknzd) ## time user tweet ## 1 2022-02-25 18:10:59 peekay14 1 ## 2 2022-02-25 18:10:59 peekay14 1 ## 3 2022-02-25 18:10:59 peekay14 1 ## 4 2022-02-25 18:10:59 peekay14 1 ## 5 2022-02-25 18:10:59 peekay14 1 ## 6 2022-02-25 18:10:59 peekay14 1 ## text ## 1 @vonderleyen @EU_Commission rubbish! Your words and those of your fellow EU politicians are baseless and useless to the people of Ukraine! Do something to help them, more severe sanctions against Russia or send in troops, but stop talking and do something, innocent people are being attacked in Europe!! ## 2 @vonderleyen @EU_Commission rubbish! Your words and those of your fellow EU politicians are baseless and useless to the people of Ukraine! Do something to help them, more severe sanctions against Russia or send in troops, but stop talking and do something, innocent people are being attacked in Europe!! ## 3 @vonderleyen @EU_Commission rubbish! Your words and those of your fellow EU politicians are baseless and useless to the people of Ukraine! Do something to help them, more severe sanctions against Russia or send in troops, but stop talking and do something, innocent people are being attacked in Europe!! ## 4 @vonderleyen @EU_Commission rubbish! Your words and those of your fellow EU politicians are baseless and useless to the people of Ukraine! Do something to help them, more severe sanctions against Russia or send in troops, but stop talking and do something, innocent people are being attacked in Europe!! ## 5 @vonderleyen @EU_Commission rubbish! Your words and those of your fellow EU politicians are baseless and useless to the people of Ukraine! Do something to help them, more severe sanctions against Russia or send in troops, but stop talking and do something, innocent people are being attacked in Europe!! ## 6 @vonderleyen @EU_Commission rubbish! Your words and those of your fellow EU politicians are baseless and useless to the people of Ukraine! Do something to help them, more severe sanctions against Russia or send in troops, but stop talking and do something, innocent people are being attacked in Europe!! ## word ## 1 @vonderleyen ## 2 @eucommiss ## 3 rubbish ## 4 word ## 5 fellow ## 6 eu Lets check how many unique words we have in our tweet data after all of our preprocessing efforts: paste(&quot;We are investigating &quot;, dim(data_tknzd)[1], &quot; non-unique features / words and &quot;, length(unique(data_tknzd$word)), &quot; unique features / words.&quot;) ## [1] &quot;We are investigating 120128 non-unique features / words and 18827 unique features / words.&quot; 6.2.2.4 Removing very rare/frequent words Finally, words that appear in practically every tweet or words that that appear only once or twice are not very helpful for investigating the typical word use of certain Twitter accounts. Since these words do not contribute to capturing similarities and differences in texts, it is a good idea to remove them. This process of relative pruning is quickly achieved with the quanteda package. Unfortunately, you will not be able to understand the code yet, but you will after reading the entire Tutorial, especially after reading the section on Topic modeling. For example, after reading that section, you willknow what a dfm is. For now, please install &amp; load quanteda and just use this code without really understanding it: # Install / load the quanteda package for topic modeling if(!require(quanteda)) { install.packages(&quot;quanteda&quot;); require(quanteda) } #load / install+load quanteda # turn the tidy data frame into a document-feature matrix: dfm &lt;- data_tknzd %&gt;% select(tweet, text, word, user) %&gt;% count(tweet, word, sort = TRUE) %&gt;% cast_dfm(tweet, word, n) # perform relative pruning: dfm &lt;- dfm_trim(dfm, min_docfreq = 0.005, # remove all words that occur in less than 0.5% of all tweets max_docfreq = 0.90, # remove all words that occur in more than 90% of all tweets docfreq_type = &quot;prop&quot;, # use probabilities verbose = FALSE) # don&#39;t tell us what you have achieved with relative pruning # turn the document-feature-matrix back into a tidy data frame: data_tknzd2 &lt;- tidy(dfm) %&gt;% rename(word = term) %&gt;% # rename columns so that their names from data_tknzd and data_tknzd2 match rename(tweet = document) %&gt;% select(-count) %&gt;% # remove the count column mutate(tweet = as.integer(tweet)) # delete the words that quanteda suggested from the original data_tknzd data_tknzd &lt;- right_join(data_tknzd, data_tknzd2, by = c(&quot;tweet&quot;,&quot;word&quot;)) %&gt;% # keep only the shorter data set without frequent/rare words, but fill in all other columns like user and date distinct(tweet,word, .keep_all= TRUE) # remove duplicate rows that have been created during the merging process Finally, now that we have removed very rare and very frequent words, lets check the number of our unique words again: paste(&quot;We are investigating &quot;, dim(data_tknzd)[1], &quot; non-unique features / words and &quot;, length(unique(data_tknzd$word)), &quot; unique features / words.&quot;) ## [1] &quot;We are investigating 55348 non-unique features / words and 346 unique features / words.&quot; Great, we have finished the text normalization process! I think we are ready to take a sneak peek at our most common words! What are the 10 most commonly used words in our tweet data? Im excited! data_tknzd %&gt;% count(word, sort = TRUE) %&gt;% slice_head(n=10) ## word n ## 1 ukrain 7555 ## 2 russia 2115 ## 3 putin 1478 ## 4 #ukrain 1364 ## 5 russian 1241 ## 6 peopl 1114 ## 7 war 1092 ## 8 nato 829 ## 9 invas 692 ## 10 countri 656 Evaluation: Shortly after Russias invasion of Ukraine began, Twitter conversations centered on the countries Ukraine, Russia, Putin, the people, the war, NATO, and the invasion. 6.2.3 (Relative) word frequencies What words are used by which Twitter accounts might reveal a lot about their agenda or thought processes. Words have power, especially in times of conflict. Words, for example, might help define who is seen as the attacker and who is seen as the defender. It matters whether I call something an attack or a war, since an aggression is a unilateral act of invasion, while a war is a reciprocal relationship that has two parties involved. Therefore, we want to look at the absolute word frequencies of the 5 most active Twitter accounts in our data set. For this, we need to group our data by the most active Twitter accounts and count how many times each user used each term. data_tknzd %&gt;% group_by(user) %&gt;% summarize(n = n()) %&gt;% arrange(desc(n)) %&gt;% filter(n &gt; 102) ## # A tibble: 6 x 2 ## user n ## &lt;chr&gt; &lt;int&gt; ## 1 DaveClubMember1 390 ## 2 common_sense54 367 ## 3 EdCutter1 251 ## 4 Seyahsm2 209 ## 5 BaloIreen 154 ## 6 LateNighter5 112 Evaluation: The five most active Twitter accounts are DaveClubMember1, common_sense54, EdCutter1, Seyahsm2, and BaloIreen, LateNighter5. frequency &lt;- data_tknzd %&gt;% filter(user == &quot;DaveClubMember1&quot; | user == &quot;common_sense54&quot; | user == &quot;EdCutter1&quot; | user == &quot;Seyahsm2&quot; | user == &quot;BaloIreen&quot; | user == &quot;LateNighter5&quot;) %&gt;% # remove all users that are not the most prominent users count(user, word, sort = TRUE) # sorts the word, i.e., the most common words are displayed first frequency ## user word n ## 1 DaveClubMember1 #prayforukrain 30 ## 2 DaveClubMember1 #stopputin 30 ## 3 DaveClubMember1 horribl 30 ## 4 DaveClubMember1 imag 30 ## 5 DaveClubMember1 join 30 ## 6 DaveClubMember1 peopl 30 ## 7 DaveClubMember1 russia 30 ## 8 DaveClubMember1 send 30 ## 9 DaveClubMember1 support 30 ## 10 DaveClubMember1 talk 30 ## 11 DaveClubMember1 ukrain 30 ## 12 DaveClubMember1 video 30 ## 13 DaveClubMember1 war 30 ## 14 common_sense54 alli 26 ## 15 common_sense54 biden 26 ## 16 common_sense54 complet 26 ## 17 common_sense54 defens 26 ## 18 common_sense54 democrat 26 ## 19 common_sense54 invas 26 ## 20 common_sense54 justifi 26 ## 21 common_sense54 leav 26 ## 22 common_sense54 peopl 26 ## 23 common_sense54 putin 26 ## 24 common_sense54 russian 26 ## 25 common_sense54 support 26 ## 26 common_sense54 ukrain 26 ## 27 common_sense54 world 26 ## 28 EdCutter1 administr 19 ## 29 EdCutter1 amp 19 ## 30 EdCutter1 biden 19 ## 31 EdCutter1 border 19 ## 32 EdCutter1 citi 19 ## 33 EdCutter1 crime 19 ## 34 EdCutter1 democrat 19 ## 35 EdCutter1 energi 19 ## 36 EdCutter1 price 19 ## 37 EdCutter1 result 19 ## 38 EdCutter1 rise 19 ## 39 EdCutter1 support 19 ## 40 EdCutter1 ukrain 19 ## 41 Seyahsm2 action 16 ## 42 Seyahsm2 america 16 ## 43 Seyahsm2 american 16 ## 44 Seyahsm2 border 16 ## 45 Seyahsm2 hope 16 ## 46 Seyahsm2 invad 16 ## 47 Seyahsm2 militari 16 ## 48 Seyahsm2 nato 16 ## 49 Seyahsm2 prepar 16 ## 50 Seyahsm2 russia 16 ## 51 Seyahsm2 secur 16 ## 52 Seyahsm2 time 16 ## 53 Seyahsm2 ukrain 16 ## 54 BaloIreen ban 10 ## 55 BaloIreen putin 10 ## 56 BaloIreen russia 10 ## 57 BaloIreen sanction 10 ## 58 BaloIreen shelter 10 ## 59 BaloIreen sky 10 ## 60 BaloIreen swift 10 ## 61 BaloIreen target 10 ## 62 BaloIreen ukrain 10 ## 63 BaloIreen ukrainian 10 ## 64 BaloIreen care 9 ## 65 BaloIreen civilian 9 ## 66 BaloIreen militari 9 ## 67 BaloIreen protect 9 ## 68 BaloIreen russian 9 ## 69 BaloIreen @potu 8 ## 70 LateNighter5 busi 8 ## 71 LateNighter5 call 8 ## 72 LateNighter5 democrat 8 ## 73 LateNighter5 die 8 ## 74 LateNighter5 fund 8 ## 75 LateNighter5 geniu 8 ## 76 LateNighter5 invas 8 ## 77 LateNighter5 republican 8 ## 78 LateNighter5 russian 8 ## 79 LateNighter5 save 8 ## 80 LateNighter5 support 8 ## 81 LateNighter5 trump 8 ## 82 LateNighter5 ukrain 8 ## 83 LateNighter5 ukrainian 8 ## 84 EdCutter1 @potu 4 ## 85 common_sense54 @foxnew 3 ## 86 BaloIreen @joebiden 1 ## 87 Seyahsm2 @potu 1 Evaluation: This is insightful. As we can see, the PorterStemmer treats ukranian and ukrain, american and america, as well as russian and russia as separate words. Since the part of speech (adjective or noun) is not particularly relevant for our analyses, we should merge the terms to focus on more meaningful TopWords. In addition, treating #ukrain and ukrain as separate words might not be hlpful. data_tknzd$word &lt;- str_replace_all(data_tknzd$word, c(&quot;russian&quot; = &quot;russia&quot;, &quot;ukrainian&quot; = &quot;ukrain&quot;, &quot;american&quot; = &quot;america&quot;, &quot;#ukrain&quot; = &quot;ukrain&quot;, &quot;#russia&quot; = &quot;russia&quot;)) frequency &lt;- data_tknzd %&gt;% filter(user == &quot;DaveClubMember1&quot; | user == &quot;common_sense54&quot; | user == &quot;EdCutter1&quot; | user == &quot;Seyahsm2&quot; | user == &quot;BaloIreen&quot; | user == &quot;LateNighter5&quot;) %&gt;% # remove all users that are not the most prominent users count(user, word, sort = TRUE) # sorts the word, i.e., the most common words are displayed first frequency ## user word n ## 1 Seyahsm2 america 32 ## 2 DaveClubMember1 #prayforukrain 30 ## 3 DaveClubMember1 #stopputin 30 ## 4 DaveClubMember1 horribl 30 ## 5 DaveClubMember1 imag 30 ## 6 DaveClubMember1 join 30 ## 7 DaveClubMember1 peopl 30 ## 8 DaveClubMember1 russia 30 ## 9 DaveClubMember1 send 30 ## 10 DaveClubMember1 support 30 ## 11 DaveClubMember1 talk 30 ## 12 DaveClubMember1 ukrain 30 ## 13 DaveClubMember1 video 30 ## 14 DaveClubMember1 war 30 ## 15 common_sense54 alli 26 ## 16 common_sense54 biden 26 ## 17 common_sense54 complet 26 ## 18 common_sense54 defens 26 ## 19 common_sense54 democrat 26 ## 20 common_sense54 invas 26 ## 21 common_sense54 justifi 26 ## 22 common_sense54 leav 26 ## 23 common_sense54 peopl 26 ## 24 common_sense54 putin 26 ## 25 common_sense54 russia 26 ## 26 common_sense54 support 26 ## 27 common_sense54 ukrain 26 ## 28 common_sense54 world 26 ## 29 BaloIreen ukrain 20 ## 30 BaloIreen russia 19 ## 31 EdCutter1 administr 19 ## 32 EdCutter1 amp 19 ## 33 EdCutter1 biden 19 ## 34 EdCutter1 border 19 ## 35 EdCutter1 citi 19 ## 36 EdCutter1 crime 19 ## 37 EdCutter1 democrat 19 ## 38 EdCutter1 energi 19 ## 39 EdCutter1 price 19 ## 40 EdCutter1 result 19 ## 41 EdCutter1 rise 19 ## 42 EdCutter1 support 19 ## 43 EdCutter1 ukrain 19 ## 44 LateNighter5 ukrain 16 ## 45 Seyahsm2 action 16 ## 46 Seyahsm2 border 16 ## 47 Seyahsm2 hope 16 ## 48 Seyahsm2 invad 16 ## 49 Seyahsm2 militari 16 ## 50 Seyahsm2 nato 16 ## 51 Seyahsm2 prepar 16 ## 52 Seyahsm2 russia 16 ## 53 Seyahsm2 secur 16 ## 54 Seyahsm2 time 16 ## 55 Seyahsm2 ukrain 16 ## 56 BaloIreen ban 10 ## 57 BaloIreen putin 10 ## 58 BaloIreen sanction 10 ## 59 BaloIreen shelter 10 ## 60 BaloIreen sky 10 ## 61 BaloIreen swift 10 ## 62 BaloIreen target 10 ## 63 BaloIreen care 9 ## 64 BaloIreen civilian 9 ## 65 BaloIreen militari 9 ## 66 BaloIreen protect 9 ## 67 BaloIreen @potu 8 ## 68 LateNighter5 busi 8 ## 69 LateNighter5 call 8 ## 70 LateNighter5 democrat 8 ## 71 LateNighter5 die 8 ## 72 LateNighter5 fund 8 ## 73 LateNighter5 geniu 8 ## 74 LateNighter5 invas 8 ## 75 LateNighter5 republican 8 ## 76 LateNighter5 russia 8 ## 77 LateNighter5 save 8 ## 78 LateNighter5 support 8 ## 79 LateNighter5 trump 8 ## 80 EdCutter1 @potu 4 ## 81 common_sense54 @foxnew 3 ## 82 BaloIreen @joebiden 1 ## 83 Seyahsm2 @potu 1 Next, we would like now the relative word frequencies. Some twitter users might have posted a lot, while others have written little, but used some meaningful words (e.g. aggression) excessively. We want to know the share of these meaningful words as compared to the absolute number of words posted by the respective user. frequency &lt;- frequency %&gt;% left_join(data_tknzd %&gt;% count(user, name = &quot;total&quot;)) %&gt;% # total = how many words has that particular user used in total? mutate(freq = ((n/total)*100)) # freq = relative frequency of the respective word compared to the total number of words that the user has used frequency ## user word n total freq ## 1 Seyahsm2 america 32 209 15.3110048 ## 2 DaveClubMember1 #prayforukrain 30 390 7.6923077 ## 3 DaveClubMember1 #stopputin 30 390 7.6923077 ## 4 DaveClubMember1 horribl 30 390 7.6923077 ## 5 DaveClubMember1 imag 30 390 7.6923077 ## 6 DaveClubMember1 join 30 390 7.6923077 ## 7 DaveClubMember1 peopl 30 390 7.6923077 ## 8 DaveClubMember1 russia 30 390 7.6923077 ## 9 DaveClubMember1 send 30 390 7.6923077 ## 10 DaveClubMember1 support 30 390 7.6923077 ## 11 DaveClubMember1 talk 30 390 7.6923077 ## 12 DaveClubMember1 ukrain 30 390 7.6923077 ## 13 DaveClubMember1 video 30 390 7.6923077 ## 14 DaveClubMember1 war 30 390 7.6923077 ## 15 common_sense54 alli 26 367 7.0844687 ## 16 common_sense54 biden 26 367 7.0844687 ## 17 common_sense54 complet 26 367 7.0844687 ## 18 common_sense54 defens 26 367 7.0844687 ## 19 common_sense54 democrat 26 367 7.0844687 ## 20 common_sense54 invas 26 367 7.0844687 ## 21 common_sense54 justifi 26 367 7.0844687 ## 22 common_sense54 leav 26 367 7.0844687 ## 23 common_sense54 peopl 26 367 7.0844687 ## 24 common_sense54 putin 26 367 7.0844687 ## 25 common_sense54 russia 26 367 7.0844687 ## 26 common_sense54 support 26 367 7.0844687 ## 27 common_sense54 ukrain 26 367 7.0844687 ## 28 common_sense54 world 26 367 7.0844687 ## 29 BaloIreen ukrain 20 154 12.9870130 ## 30 BaloIreen russia 19 154 12.3376623 ## 31 EdCutter1 administr 19 251 7.5697211 ## 32 EdCutter1 amp 19 251 7.5697211 ## 33 EdCutter1 biden 19 251 7.5697211 ## 34 EdCutter1 border 19 251 7.5697211 ## 35 EdCutter1 citi 19 251 7.5697211 ## 36 EdCutter1 crime 19 251 7.5697211 ## 37 EdCutter1 democrat 19 251 7.5697211 ## 38 EdCutter1 energi 19 251 7.5697211 ## 39 EdCutter1 price 19 251 7.5697211 ## 40 EdCutter1 result 19 251 7.5697211 ## 41 EdCutter1 rise 19 251 7.5697211 ## 42 EdCutter1 support 19 251 7.5697211 ## 43 EdCutter1 ukrain 19 251 7.5697211 ## 44 LateNighter5 ukrain 16 112 14.2857143 ## 45 Seyahsm2 action 16 209 7.6555024 ## 46 Seyahsm2 border 16 209 7.6555024 ## 47 Seyahsm2 hope 16 209 7.6555024 ## 48 Seyahsm2 invad 16 209 7.6555024 ## 49 Seyahsm2 militari 16 209 7.6555024 ## 50 Seyahsm2 nato 16 209 7.6555024 ## 51 Seyahsm2 prepar 16 209 7.6555024 ## 52 Seyahsm2 russia 16 209 7.6555024 ## 53 Seyahsm2 secur 16 209 7.6555024 ## 54 Seyahsm2 time 16 209 7.6555024 ## 55 Seyahsm2 ukrain 16 209 7.6555024 ## 56 BaloIreen ban 10 154 6.4935065 ## 57 BaloIreen putin 10 154 6.4935065 ## 58 BaloIreen sanction 10 154 6.4935065 ## 59 BaloIreen shelter 10 154 6.4935065 ## 60 BaloIreen sky 10 154 6.4935065 ## 61 BaloIreen swift 10 154 6.4935065 ## 62 BaloIreen target 10 154 6.4935065 ## 63 BaloIreen care 9 154 5.8441558 ## 64 BaloIreen civilian 9 154 5.8441558 ## 65 BaloIreen militari 9 154 5.8441558 ## 66 BaloIreen protect 9 154 5.8441558 ## 67 BaloIreen @potu 8 154 5.1948052 ## 68 LateNighter5 busi 8 112 7.1428571 ## 69 LateNighter5 call 8 112 7.1428571 ## 70 LateNighter5 democrat 8 112 7.1428571 ## 71 LateNighter5 die 8 112 7.1428571 ## 72 LateNighter5 fund 8 112 7.1428571 ## 73 LateNighter5 geniu 8 112 7.1428571 ## 74 LateNighter5 invas 8 112 7.1428571 ## 75 LateNighter5 republican 8 112 7.1428571 ## 76 LateNighter5 russia 8 112 7.1428571 ## 77 LateNighter5 save 8 112 7.1428571 ## 78 LateNighter5 support 8 112 7.1428571 ## 79 LateNighter5 trump 8 112 7.1428571 ## 80 EdCutter1 @potu 4 251 1.5936255 ## 81 common_sense54 @foxnew 3 367 0.8174387 ## 82 BaloIreen @joebiden 1 154 0.6493506 ## 83 Seyahsm2 @potu 1 209 0.4784689 Lets make the user-specific word lists and their similarities / differences a little easier to interpret: frequency %&gt;% arrange(desc(n)) %&gt;% group_by(user) %&gt;% slice_head(n=10) %&gt;% arrange(desc(n)) %&gt;% select(user, word) %&gt;% summarize(terms = list(word)) %&gt;% mutate(terms = map(terms, paste, collapse = &quot;, &quot;)) %&gt;% unnest(cols = c(terms)) %&gt;% group_by(user) ## # A tibble: 6 x 2 ## # Groups: user [6] ## user terms ## &lt;chr&gt; &lt;chr&gt; ## 1 BaloIreen ukrain, russia, ban, putin, sanction, shelter, sky, swift, target, care ## 2 common_sense54 alli, biden, complet, defens, democrat, invas, justifi, leav, peopl, putin ## 3 DaveClubMember1 #prayforukrain, #stopputin, horribl, imag, join, peopl, russia, send, support, talk ## 4 EdCutter1 administr, amp, biden, border, citi, crime, democrat, energi, price, result ## 5 LateNighter5 ukrain, busi, call, democrat, die, fund, geniu, invas, republican, russia ## 6 Seyahsm2 america, action, border, hope, invad, militari, nato, prepar, russia, secur Evaluation: Most of the Twitter accounts seem to be U.S. American. However, they focus on different issues. (1) BaloIreen seems to demand for economic actions against Russia, (2) common_sense54 seems to focus on a Presidential / democratic alliance against Russia and to take the perspective of a military defender, (3) DaveClubMember1 seems to ask for civic aid and engagement, (4) EdCutter1 seems to tweet about a mix of issues covered by the user 4 users (economy, government action, war crimes, etc.), (5) LateNighter5 tweets could also be about businesses and Republicans, and Seyahsm2 focuses on NATO, its border control, and hope. To plot frequencies against each other and get a real good overview, we need to transform the data in a way that every user has his/her own column with her/her word frequencies because we need to use these frequencies on a y- and y-axis. Therefore, well use the pivot_wider function from the tidyr package that comes pre-installed with the tidyverse. frequency_wide &lt;- frequency %&gt;% select(user, word, freq) %&gt;% pivot_wider(names_from = user, values_from = freq) frequency_wide ## # A tibble: 61 x 7 ## word Seyahsm2 DaveClubMember1 common_sense54 BaloIreen EdCutter1 LateNighter5 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 america 15.3 NA NA NA NA NA ## 2 #prayforukrain NA 7.69 NA NA NA NA ## 3 #stopputin NA 7.69 NA NA NA NA ## 4 horribl NA 7.69 NA NA NA NA ## 5 imag NA 7.69 NA NA NA NA ## 6 join NA 7.69 NA NA NA NA ## 7 peopl NA 7.69 7.08 NA NA NA ## 8 russia 7.66 7.69 7.08 12.3 NA 7.14 ## 9 send NA 7.69 NA NA NA NA ## 10 support NA 7.69 7.08 NA 7.57 7.14 ## # ... with 51 more rows Now, lets investigate which words are typical for DaveClubMember1 compared to common_sense54. frequency_wide %&gt;% filter(!is.na(DaveClubMember1)) %&gt;% # keep only words that are being used by both Twitter accounts filter(!is.na(common_sense54)) %&gt;% ggplot(aes(x=DaveClubMember1, y=common_sense54, label=word)) + # geom_point(alpha = 0.2, size = 4, position=position_jitter(h=0.15,w=0.15)) + geom_text(hjust=0, vjust=0, check_overlap = TRUE, position=position_jitter(h=0.25,w=0.25), size=3.0) + ylim(6.0,8.5) + xlim(6.0,8.5) + geom_abline(color = &quot;#67A9B6&quot;, na.rm=TRUE) Evaluation: Both accounts have mentioned all of the words in this plot at least once in their tweets. Words along the blue line are used approximately equally by both accounts, while words further away from the line are used significantly more frequently by one account than the other. Here, all terms are more characteristic of DaveClubMember1 than of common_sense54 because they are beneath the blue line. 6.2.4 Word log odds So far, weve looked at how often a word is used compared to all the other words an account has posted (absolute &amp; relative frequencies). Next, well look at how probable it is that a word was posted by a certain account (log odds). There is only one problem: measurement error. Probabilities are more accurate for words that are used frequently and less accurate for words that have been measured only a few times. A way to correct this measurement error is to use the tidylo package that creates weighted, i.e. corrected, log odds ratio based on an approach proposed by Monroe, Colaresi, and Quinn (2008). # Install / load the tidy log odds package to create weighted log odds if(!require(tidylo)) { install.packages(&quot;tidylo&quot;); require(tidylo) } #load / install+load tidylo word_lo &lt;- data_tknzd %&gt;% filter(user == &quot;DaveClubMember1&quot; | user == &quot;common_sense54&quot; | user == &quot;EdCutter1&quot; | user == &quot;Seyahsm2&quot; | user == &quot;BaloIreen&quot; | user == &quot;LateNighter5&quot;) %&gt;% # remove all users that are not the most prominent users count(user, word, sort = TRUE) %&gt;% bind_log_odds(user, word, n, unweighted = TRUE) %&gt;% # mutate(prblty = (plogis(log_odds)*100)) %&gt;% arrange(desc(log_odds_weighted)) head(word_lo) ## user word n log_odds log_odds_weighted ## 1 Seyahsm2 america 32 2.045056 11.568584 ## 2 DaveClubMember1 #prayforukrain 30 1.550535 8.492628 ## 3 DaveClubMember1 #stopputin 30 1.550535 8.492628 ## 4 DaveClubMember1 horribl 30 1.550535 8.492628 ## 5 DaveClubMember1 imag 30 1.550535 8.492628 ## 6 DaveClubMember1 join 30 1.550535 8.492628 Log odds ratio express the likelihood that a word comes from a certain account, compared to all other accounts. As a result, the highest log odds indicate words that are extremely distinctive for an account. Using a table, lets compare the weighted log odds for some words across accounts. word_lo2 &lt;- data_tknzd %&gt;% filter(user == &quot;DaveClubMember1&quot; | user == &quot;common_sense54&quot; | user == &quot;EdCutter1&quot; | user == &quot;Seyahsm2&quot; | user == &quot;BaloIreen&quot;) %&gt;% # remove all users that are not the most prominent users count(user, word, sort = TRUE) %&gt;% bind_log_odds(user, word, n) %&gt;% select(-n) %&gt;% spread(user, log_odds_weighted, fill = 0) head(word_lo2) ## word BaloIreen common_sense54 DaveClubMember1 EdCutter1 Seyahsm2 ## 1 #prayforukrain 0.000000 0.000000 7.716397 0.000000 0.000000 ## 2 #stopputin 0.000000 0.000000 7.716397 0.000000 0.000000 ## 3 @foxnew 0.000000 2.335714 0.000000 0.000000 0.000000 ## 4 @joebiden 2.016194 0.000000 0.000000 0.000000 0.000000 ## 5 @potu 4.385952 0.000000 0.000000 2.025966 1.717495 ## 6 action 0.000000 0.000000 0.000000 0.000000 7.457153 To get a better overview, lets create a visualization of the 10 most distinctive words per account. word_lo %&gt;% group_by(user) %&gt;% arrange(desc(log_odds_weighted)) %&gt;% slice_head(n=10) %&gt;% ungroup %&gt;% mutate(word = reorder(word, log_odds_weighted)) %&gt;% ggplot(aes(word, log_odds_weighted, fill = user)) + geom_col(show.legend = FALSE) + scale_fill_manual(values = c(&quot;#9FC6C0&quot;,&quot;#89BFC1&quot;,&quot;#67A9B6&quot;,&quot;#5495AD&quot;,&quot;#377099&quot;,&quot;#08333F&quot;)) + facet_wrap(~user, scales = &quot;free&quot;) + coord_flip() + labs(y = &quot;Log Odds Ratio&quot;, x=NULL) 6.3 Topic modeling In this section, well use a combination of the tidytext package, the quanteda package, and the topicmodels package to create topic models with Latent Dirichlet allocation (LDA), one of the most prevalent topic modeling algorithms. LDA creates mixed-membership models, i.e., LDA assumes that every text contains a mix of different topics, e.g. a tweet can be 60% about TopicA (War crimes) and 40% about TopicB (Housing of refugees). Topics are defined by the mix of words that are associated with them. For example, the most common words in TopicA (War crimes) can be massacre, soldiers, and brutal. The most common words in TopicB (Housing of refugees) can be volunteers, shelter, and children. However, both topics can be mentioned in one single text, e.g., tweets about the brutal war crimes of Russian soldiers that force Ukrainian refugees to take their children and seek shelter in neighboring countries. LDA estimates the most common words in a topic and the most common topics in a text simultaneously. Image: Assigning topics to a document (Screenshot from: Chris Bail): Important hint: Both the quantedaand the topicmodels package use machine learning lingo. That is, words are called features and texts/tweets are called documents. The total sum of all documents is called a corpus. In the long run, you should get used to this lingo, so we will keep using it in this tutorial, too. 6.3.1 First steps If you havent already, please install / load the quanteda and topicmodels packages: # Install / load the quanteda package for data transformation if(!require(quanteda)) { install.packages(&quot;quanteda&quot;); require(quanteda) } #load / install+load quanteda # Install / load the topicmodels package for topic modeling if(!require(topicmodels)) { install.packages(&quot;topicmodels&quot;); require(topicmodels) } #load / install+load topicmodels Next, using the tidytext package, we will convert our tidy text data into a document-feature matrix (dfm) that the topicmodels package can understand and do calculations with. In a dfm rows represent the documents (= texts), columns represent features (= unique words), the cells represent the frequency with which a feature appears in a specific document. # Cast the tidy text data into a matrix (dfm) that topicmodels can use for calculation: dfm &lt;- data_tknzd %&gt;% select(tweet, text, word) %&gt;% count(tweet, word, sort = TRUE) %&gt;% cast_dfm(tweet, word, n) head(dfm) ## Document-feature matrix of: 6 documents, 338 features (97.78% sparse) and 0 docvars. ## features ## docs ukrain russia america russiaukrain attack eu europ innoc peopl sanction ## 231 3 1 0 0 0 0 0 0 0 0 ## 246 3 0 0 0 0 0 0 0 0 0 ## 614 3 1 0 0 0 0 0 0 0 0 ## 4485 1 3 0 0 0 0 0 0 0 0 ## 4527 2 3 0 0 0 0 0 0 0 0 ## 5431 3 1 0 0 0 0 0 0 0 0 ## [ reached max_nfeat ... 328 more features ] Lets check out the dimensions of this new document-feature matrix: paste(&quot;We are investigating &quot;, dim(dfm)[1], &quot; documents (tweets) and &quot;, dim(dfm)[2], &quot; features (unique words).&quot;) ## [1] &quot;We are investigating 9589 documents (tweets) and 338 features (unique words).&quot; And lets have a look at our top features: quanteda::topfeatures(dfm, 10) # this is a neat function from quanteda to investigate the top features in a dfm ## ukrain russia putin peopl war nato invas countri support world ## 9561 3777 1478 1114 1092 829 692 656 623 579 6.3.2 Model estimation: Number of topics As a researcher, you must first estimate the number of topics you anticipate to encounter across all documents (= the number of topics K in a corpus) before fitting an LDA model. If you estimate there are approximately 20 topics that Twitter accounts, for example, youll set K = 20 to extract 20 separate topics. The 20 topics are then extracted from the corpus based on the distribution of co-occurring words in each document. Choosing a good value for K is extremely important and consequential, since it will impact your results. Usually, small Ks produce very distinct, but generalizable topics, while high Ks produce overlapping themes, but are also more event- and issue-specific. Lets create a topic model. More specifically, lets create a six-topic LDA model using topicmodels. In practice, you would often use a higher K, but for our use case, three should suffice. lda &lt;- LDA(dfm, k = 3, control = list(seed = 123)) # the control argument uses a random number (123) to seed the assignment of topics to each word in the corpus (this helps to create reproducability in an otherwise random process!) 6.3.3 Inspect the topics 6.3.3.1 Word-topic probabilities To inspect the topics with the tidytext package, we need to tidy up our LDA model first, i.e., bring it back into a data format that works for tidy text analysis. # turn the lda model back into a tidy data frame: tidy_lda &lt;- tidy(lda, matrix = &quot;beta&quot;) %&gt;% # matrix = &quot;beta&quot; creates the word-topic probabilities rename(word = term) head(tidy_lda) ## # A tibble: 6 x 3 ## topic word beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 ukrain 0.310 ## 2 2 ukrain 0.127 ## 3 3 ukrain 0.0820 ## 4 1 russia 0.00513 ## 5 2 russia 0.194 ## 6 3 russia 0.00558 The new column,  (beta), shows the per-topic-per-word probabilities, i.e., the probability that the respective feature / word is being generated from the topic under examination. To put it another way, its the probability that a feature is common in a certain topic. The word-topic matrix is often used to analyze and label topics (i.e., using the features with the highest conditional probability for that topic). In summary, the word-topic matrix aids in the creation of topic-specific word lists. Lets create a visualization of the 10 terms that are most common within each topic. tidy_lda %&gt;% group_by(topic) %&gt;% arrange(desc(beta)) %&gt;% slice_head(n=10) %&gt;% ungroup() %&gt;% ggplot(aes(reorder(word, beta), y=beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + scale_fill_manual(values = c(&quot;#9FC6C0&quot;,&quot;#5495AD&quot;,&quot;#08333F&quot;)) + ylim(0,0.4) + facet_wrap(~topic, scales=&quot;free&quot;, labeller = as_labeller(c(`1` = &quot;Topic 1&quot;, `2` = &quot;Topic 2&quot;, `3` = &quot;Topic 3&quot;))) + xlab(&quot;word&quot;) + coord_flip() As we can see, the topics are note really exclusive, i.e., have a lot of overlaps. This is a sign that we (a) need more preprocessing / text normalization and (b) need to adjust the number of K topics. However, topic modeling is usually more difficult for tweets than for news texts, so its unclear whether well be able to significantly enhance the results anyway. Topic 1: Seems to have the focus on Ukraine as the defender. Ukraine is being invaded by Russia, it is forced into a war, it needs support, etc. Ukraine is the protagonist in this topic. Topic 2: Seems to have the focus on Russia as the aggressor. Russia is invading Ukraine, and it needs to be stopped by U.S. America / the West. Russia is the protagonist of this topic. Topic 3: Seems to focus on the Ukrainian people and how Putin is threatening their lives. The Ukrainian people is the protagonist of this topic. Now that we know who the protagonist of each topic is, lets take a look at the less frequent terms in each topic to check if our interpretation of the topics remains logical. Lets have a look at the issues without the top performers ukrain, russia, and putin and look at the topics again. tidy_lda %&gt;% filter(word!=&quot;putin&quot;, word!=&quot;ukrain&quot;, word!=&quot;russia&quot;) %&gt;% group_by(topic) %&gt;% arrange(desc(beta)) %&gt;% slice_head(n=10) %&gt;% ungroup() %&gt;% ggplot(aes(x=reorder(word, beta), y=beta, fill = factor(topic))) + geom_col(show.legend = FALSE, stat = &quot;identity&quot;) + scale_fill_manual(values = c(&quot;#9FC6C0&quot;,&quot;#5495AD&quot;,&quot;#08333F&quot;)) + ylim(0,0.07) + facet_wrap(~topic, scales=&quot;free&quot;, labeller = as_labeller(c(`1` = &quot;Topic 1&quot;, `2` = &quot;Topic 2&quot;, `3` = &quot;Topic 3&quot;))) + xlab(&quot;word&quot;) + coord_flip() Despite the fact that the topics are not mutually exclusive, they appear to be a reasonable approximation of a first tweet categorization. The accuracy of this categorization must be confirmed, particularly by extensive / deep reading. Remember that topics are only suggestions that should be complemented with more sophisticated (often: qualitative) techniques. The high level of overlap across topics might possibly be attributed to the fact that the dataset includes tweets from extremely comparable situations. All tweets were created on February 25 (the first day following the outbreak of Russian aggression against Ukraine) between 18:00 and 18:10 Paris time, i.e., all Tweets are, thematically speaking, about the Russian invasion of Ukraine. Naturally, the subjects of these tweets are fairly similar, which is to be anticipated in these circumstances. Thus, the three emerging topics serve (a little bit!) as frames of the same event since the conditions of their origin are so similar (namely the outbreak of Russian aggression). When understanding topics as frames, however, one should not go overboard (remember, e.g., that we are investigating multi-membership models)! See Nicholls &amp; Culpepper (2020) for a full explanation of why topics and frames are not the same thing, i.e., why topic modeling should not be used for framing analysis. 6.3.3.2 Document-topic probabilities Now that we know which words are associated with what topics, we also want to know what documents (i.e. tweets) are associated with what topics. # again, turn the lda model back into a tidy data frame: tidy_lda2 &lt;- tidy(lda, matrix = &quot;gamma&quot;) # matrix = &quot;gamma&quot; creates the document-topic probabilities head(tidy_lda2) ## # A tibble: 6 x 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 231 1 0.328 ## 2 246 1 0.346 ## 3 614 1 0.336 ## 4 4485 1 0.326 ## 5 4527 1 0.330 ## 6 5431 1 0.333 The new column,  (gamma), shows the per-document-per-topic probabilities, i.e., the proportion of words from that document that are generated from the topic under examination. To put it another way, its the probability that a topic is common in a certain document. The document-topic matrix is used to identify the top documents of a topic (i.e., using the documents with the highest probability for that topic) and to assign main topics to documents In summary, the word-topic matrix aids in the creation of document-specific topic lists. Lets investigate which documents have the highest probability for Topic 2, the topic that seems to focus on Russia as the aggressor. tidy_lda2 %&gt;% filter(topic == 2) %&gt;% arrange(desc(gamma)) ## # A tibble: 9,589 x 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 19634 2 0.361 ## 2 19659 2 0.361 ## 3 18418 2 0.358 ## 4 19362 2 0.357 ## 5 9189 2 0.356 ## 6 41135 2 0.356 ## 7 2589 2 0.355 ## 8 29143 2 0.355 ## 9 6832 2 0.354 ## 10 40393 2 0.354 ## # ... with 9,579 more rows Evaluation: 36.1% of document 19634, i.e. tweet No. 19634, are related to Topic2. This is also true for document 19659. Lets have a look aboth tweets and evaluate their word choice and full tweet text. data_tknzd %&gt;% select(tweet, text, word) %&gt;% filter(tweet == 19634) %&gt;% filter(row_number()==1) ## tweet ## 1 19634 ## text ## 1 @Salem4Congress I wouldn&#39;t considered what happened in Ukraine to be a &quot;violent coup&quot; especially given Russia is now invading the pro Ukraine parts of Ukraine.\\n\\nI respect you but we simply must be principled and call out Russian imperialism just as we should also call out American imperialism. ## word ## 1 happen data_tknzd %&gt;% select(tweet, text, word) %&gt;% filter(tweet == 19659) %&gt;% filter(row_number()==1) ## tweet ## 1 19659 ## text ## 1 Russia next target\\nFinland and Sweden\\n\\n#Finland #Sweden\\n#Ukraine #Russia #WWIII #worldwar3 #war #WW3 #Putin #PrayForUkraine #RussiaUkraineConflict #RussiaUkraine #Russian #UkraineCrisis #Ukraina #UkraineInvasion #RussiaUkraineWar #NATO #UkraineUnderAttack ## word ## 1 russia Both tweets are about Russian imperialism (thats a pretty nice title for Topic 2, by the way). Topic 2 seems like an okay fit. For comparison, lets also look at documents that score high on Topic 1. tidy_lda2 %&gt;% filter(topic == 1) %&gt;% arrange(desc(gamma)) ## # A tibble: 9,589 x 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 25909 1 0.353 ## 2 37798 1 0.350 ## 3 33712 1 0.350 ## 4 9577 1 0.350 ## 5 2525 1 0.350 ## 6 48632 1 0.349 ## 7 13720 1 0.349 ## 8 12049 1 0.348 ## 9 19055 1 0.348 ## 10 43651 1 0.348 ## # ... with 9,579 more rows Both tweet No. 25909 and No. 37798 have a high share of topic 1. data_tknzd %&gt;% select(tweet, text, word) %&gt;% filter(tweet == 25909) %&gt;% filter(row_number()==1) ## tweet ## 1 25909 ## text ## 1 Students from Ukraine sending distress messages. I am so helpless at the moment. Anyone who knows anyone in India, can you please contact the Indian authorities to know if they can get an escort from east Ukraine to western border? #Ukraine https://t.co/OidzCQg5XI ## word ## 1 student data_tknzd %&gt;% select(tweet, text, word) %&gt;% filter(tweet == 37798) %&gt;% filter(row_number()==1) ## tweet ## 1 37798 ## text ## 1 Current tally of Ukrainian victims: 137 reported dead. Over 300 wounded. Please @POTUS! We understand why you do not want to send US troops directly into Ukraine, but is there nothing more we can do? &lt;U+0001F62A&gt;&lt;U+0001F43E&gt; ## word ## 1 current These tweets seem to be about Ukrainian victims and war casualties. That comes close to our interpretation of topic 1. This lead us to a final question, which is answered in the next section. 6.3.3.3 Assessment of the topics quality How do you assess the quality of proposed topic models? Use the approach recommended by Grimmer, Roberts &amp; Steward (2022, p. 152) when you want to assess the quality of your topic models. Read a random sample of documents allocated to a topic carefully, but keep in mind that documents are partial members of all topics. Therefore, the authors advise looking at documents that have a high proportion of words that are associated with the topic under examination. That is, one should create a small subset of documents where the largest portion of the document is associated with the particular topic of interest. Go over these documents to see what they have in common and whether the proposed topic makes sense from an organizational standpoint. Building on code provided by Ian T. Adams, we can create a beautiful overview of our extracted topics and the most common words that contribute to each topic: top_terms &lt;- tidy_lda %&gt;% arrange(desc(beta)) %&gt;% group_by(topic) %&gt;% slice_head(n=10) %&gt;% arrange(desc(beta)) %&gt;% select(topic, word) %&gt;% summarize(terms = list(word)) %&gt;% mutate(terms = map(terms, paste, collapse = &quot;, &quot;)) %&gt;% unnest(cols = c(terms)) gamma_terms &lt;- tidy_lda2 %&gt;% group_by(topic) %&gt;% summarize(gamma = mean(gamma)) %&gt;% arrange(desc(gamma)) %&gt;% left_join(top_terms, by = &quot;topic&quot;) %&gt;% mutate(topic = paste0(&quot;Topic &quot;, topic), topic = reorder(topic, gamma)) gamma_terms %&gt;% arrange(desc(gamma)) %&gt;% slice_head(n=10) %&gt;% mutate(topic = factor(topic, levels = c(&quot;Topic 1&quot;,&quot;Topic 2&quot;,&quot;Topic 3&quot;))) %&gt;% ggplot(aes(topic, gamma, label = terms, fill = topic)) + geom_col(show.legend = FALSE) + geom_text(hjust = 1.1, nudge_y = 0.0005, size = 3, color=&quot;white&quot;) + scale_fill_manual(values = c(&quot;#9FC6C0&quot;,&quot;#5495AD&quot;,&quot;#08333F&quot;)) + coord_flip() + theme_bw() + theme(plot.title = element_text(size = 14)) + labs(x = NULL, y = expression(gamma), title = &quot;The Three Topics In The Ukrainian Twitter Data (25.02)&quot;, subtitle = &quot;With the top words that contribute to each topic&quot;) Or in a table format: library(knitr) gamma_terms %&gt;% mutate(topic = factor(topic, levels = c(&quot;Topic 1&quot;,&quot;Topic 2&quot;,&quot;Topic 3&quot;))) %&gt;% arrange(topic) %&gt;% select(topic, gamma, terms) %&gt;% kable(digits = 3, col.names = c(&quot;Topic&quot;, &quot;Expected topic proportion&quot;, &quot;Top 6 terms&quot;)) Topic Expected topic proportion Top 6 terms Topic 1 0.333 ukrain, war, invas, support, putin, world, send, nato, happen, forc Topic 2 0.333 russia, ukrain, invad, putin, nato, stop, america, invas, militari, time Topic 3 0.333 ukrain, peopl, putin, countri, war, nato, amp, presid, live, sanction Final remark. While the quality of our three topics may appear satisfactory to you (or perhaps not?), we must remember that our choice of K was purely arbitrary. Post-hoc rationalization of this choice is inherently problematic. If possible, you should base your choice of K on previous literature and what other researchers have said about a suitable K for your corpus / your research question. If no such information is available, the searchK function of the stm package is your final resort, but it must always be accompanied by critical re-reading and examination of the topics and documents (e.g., semantic coherence and exclusivity of topics).4 6.3.4 STMs So far, we have only used the text of the tweet to estimate the topics. However, the creation date of a tweet could also be an important indicator of a topic, as some topics are more prevalent at certain points in time (the same applies to the author of a tweet, of course). While LDA has been the most famous topic modeling algorithm, there are currently a variety of similar techniques available, which all advance the field. One very popular technique is Structural Topic Modeling (STM), which is very similar to LDA, but uses meta data about documents (e.g., author name and creation date) to enhance word assignment to latent topics in the corpus. You can estimate STMs with the stm package. We will not discuss STMs in this tutorial due to time restrictions. However, the Additional tutorials section features two excellent external STM tutorials that you might want to have a look at. 6.4 Take-Aways Text normalization: You can perform the entire preprocessing of your documents using the tidy text approach (tokenization, stop words, removal, etc.). Word freqencies &amp; Log Odds: The tidy text approach can also be used to calculate word frequencies and log odds to identify the most common word per author, etc. Topic modeling: Topic models are mixed-membership models, i.e., they assume that every text contains a mix of different topics. The best known topic modeling algorithm is LDA. K: Number of topics to be estimated. Word-topic matrix: Aids in the creation of topic-specific word lists. Document-topic matrix: Aids in the creation of document-specific topic lists. 6.5 Additional tutorials You still have questions? The following tutorials, books, &amp; papers may help you: Text as Data by V. Hase, Tutorial 13 Topic Modeling by C. Bail Text Mining in R. A Tidy Approach by Silge &amp; Robinson (2017) To disable this feature, use the to_lower = FALSE option. This function takes a range of K values, calculates topic models for each value of K, and then generates goodness-of-fit measurements that indicate what value for K fits the data best. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
